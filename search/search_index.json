{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\ude99 cgml","text":"<p>Welcome to the cgml repository, created for the <code>Fraud-Detection using Graphs</code> challenge! \ud83d\ude80  </p> <ul> <li>\ud83d\udcd6 Read the full blog post about this project here.  </li> <li>\ud83d\udcda Explore the MkDocs documentation here.  </li> </ul>"},{"location":"#installing-uv","title":"\u2699\ufe0f Installing <code>uv</code>","text":"<p><code>uv</code> is a universal runtime tool that simplifies running and managing Python applications with cross-platform compatibility.  </p>"},{"location":"#macoslinux","title":"\ud83d\udc27 macOS/Linux","text":"<p>Run the following command in your terminal: <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p>"},{"location":"#windows","title":"\ud83d\udda5\ufe0f Windows","text":"<p>Run this command in PowerShell: <pre><code>powershell -ExecutionPolicy ByPass -Command \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p>"},{"location":"#setting-up-the-environment","title":"\ud83c\udf31 Setting Up the Environment","text":"<p>To set up your development environment, create a virtual environment and install all dependencies: <pre><code>make install\n</code></pre></p> <p>Notes: - Ensure <code>make</code> is installed on your system. - Windows users can install <code>make</code> via tools like <code>choco</code> or <code>winget</code>.  </p>"},{"location":"#mkdocs-documentation","title":"\ud83d\udcdd MkDocs Documentation","text":"<p>Start the MkDocs server locally with: <pre><code>make docs-serve\n</code></pre></p>"},{"location":"#further-instructions","title":"\ud83d\udee0\ufe0f Further Instructions","text":"<ol> <li>Duplicate the <code>.env-example</code> file as <code>.env</code> and fill in the required environment variables \ud83d\udd11.  </li> <li>Load the environment variables: <pre><code>source .env\n</code></pre></li> <li>You're ready to start! \ud83c\udf89  </li> </ol>"},{"location":"#project-structure","title":"\ud83d\udcc2 Project Structure","text":"<pre><code>\u251c\u2500\u2500 .github/workflows         &lt;- GitHub Actions workflows.  \n\u251c\u2500\u2500 data       \n\u2502   \u251c\u2500\u2500 processed             &lt;- Final datasets ready for modeling.  \n\u2502   \u2514\u2500\u2500 raw                   &lt;- Original, immutable data.  \n\u2502       \n\u251c\u2500\u2500 docs                      &lt;- Project documentation.  \n\u251c\u2500\u2500 notebooks                 &lt;- Jupyter or Quarto Markdown notebooks.  \n\u2502                                Naming: `00-description.qmd`.  \n\u2502        \n\u251c\u2500\u2500 reports                   &lt;- Generated outputs: HTML, PDF, diagrams, etc.  \n\u251c\u2500\u2500 src/CryptoFraudDetection  &lt;- Core source code for the project.  \n\u251c\u2500\u2500 tests                     &lt;- Unit tests.  \n\u251c\u2500\u2500 .env-example              &lt;- Sample environment variables.  \n\u251c\u2500\u2500 .gitignore                &lt;- Files ignored by git.  \n\u251c\u2500\u2500 LICENSE                   &lt;- MIT License.  \n\u251c\u2500\u2500 Makefile                  &lt;- Commands like `make install` and `make test`.  \n\u251c\u2500\u2500 mkdocs.yaml               &lt;- MkDocs configuration.  \n\u251c\u2500\u2500 pyproject.toml            &lt;- Build configuration.  \n\u251c\u2500\u2500 README.md                 &lt;- This README file.  \n\u251c\u2500\u2500 ruff.toml                 &lt;- Ruff configuration.  \n\u2514\u2500\u2500 uv.lock                   &lt;- Dependency lock file.  \n</code></pre>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>This project is licensed under the MIT License.  </p>"},{"location":"CryptoFraudDetection/elasticsearch/data_insertion/","title":"data_insertion","text":"<p>File: data_insertion.py.</p> Description <p>This file is used to insert data into Elasticsearch.</p>"},{"location":"CryptoFraudDetection/elasticsearch/data_insertion/#src.CryptoFraudDetection.elasticsearch.data_insertion.insert_dataframe","title":"<code>insert_dataframe(logger, index, df)</code>","text":"<p>Insert a pandas DataFrame into Elasticsearch.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>The logger object.</p> <code>index</code> <code>str</code> <p>The name of the Elasticsearch index to insert into.</p> <code>df</code> <code>DataFrame</code> <p>The DataFrame to insert.</p> <p>Returns:</p> Name Type Description <code>response</code> <code>tuple[int, int | list[dict[str, list]]]</code> <p>The response from the Elasticsearch bulk insertion.</p> Source code in <code>src/CryptoFraudDetection/elasticsearch/data_insertion.py</code> <pre><code>def insert_dataframe(\n    logger: Logger,\n    index: str,\n    df: pd.DataFrame,\n) -&gt; tuple[int, int | list[dict[str, list]]]:\n    \"\"\"\n    Insert a pandas DataFrame into Elasticsearch.\n\n    Attributes:\n        logger (Logger): The logger object.\n        index (str): The name of the Elasticsearch index to insert into.\n        df (pd.DataFrame): The DataFrame to insert.\n\n    Returns:\n        response (tuple[int, int | list[dict[str, list]]]): The response from the Elasticsearch bulk insertion.\n\n    \"\"\"\n    if \"id\" in df.columns:\n        data = [\n            {\n                \"_index\": index,\n                \"_id\": record[\"id\"],\n                \"_op_type\": \"create\",\n                \"_source\": record,\n            }\n            for record in df.to_dict(orient=\"records\")\n        ]\n    else:\n        data = [\n            {\n                \"_index\": index,\n                \"_source\": record,\n            }\n            for record in df.to_dict(orient=\"records\")\n        ]\n\n    try:\n        logger.info(f\"Inserting {len(data)} records into {index}...\")\n        response = bulk(client=es, actions=data, raise_on_error=False)\n    except BulkIndexError as e:\n        logger.handle_exception(\n            BulkIndexError,\n            f\"Skipped some documents:\\n{e}\",\n        )\n    return response\n</code></pre>"},{"location":"CryptoFraudDetection/elasticsearch/data_insertion/#src.CryptoFraudDetection.elasticsearch.data_insertion.insert_dict","title":"<code>insert_dict(logger, index, data_dict)</code>","text":"<p>Insert a list of dictionaries into Elasticsearch.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>The logger object.</p> <code>index</code> <code>str</code> <p>The name of the Elasticsearch index to insert into.</p> <code>data_dict</code> <code>dict[str, list[str]]</code> <p>The dictionary to insert.</p> <p>Returns:</p> Name Type Description <code>response</code> <code>tuple[int, int | list[dict[str, Any]]</code> <p>The response from the Elasticsearch bulk insertion.</p> Source code in <code>src/CryptoFraudDetection/elasticsearch/data_insertion.py</code> <pre><code>def insert_dict(\n    logger: Logger,\n    index: str,\n    data_dict: dict[str, list[str]],\n) -&gt; tuple[int, int | list[dict[str, Any]]]:\n    \"\"\"\n    Insert a list of dictionaries into Elasticsearch.\n\n    Attributes:\n        logger (Logger): The logger object.\n        index (str): The name of the Elasticsearch index to insert into.\n        data_dict (dict[str, list[str]]): The dictionary to insert.\n\n    Returns:\n        response (tuple[int, int | list[dict[str, Any]]): The response from the Elasticsearch bulk insertion.\n\n    \"\"\"\n    data_frame = pd.DataFrame(data_dict)\n    return insert_dataframe(logger, index, data_frame)\n</code></pre>"},{"location":"CryptoFraudDetection/elasticsearch/data_retrieval/","title":"data_retrieval","text":"<p>File: data_retrieval.py.</p> Description <p>This file is used to retrieve data from Elasticsearch.</p>"},{"location":"CryptoFraudDetection/elasticsearch/data_retrieval/#src.CryptoFraudDetection.elasticsearch.data_retrieval.search_data","title":"<code>search_data(index, q, size=None)</code>","text":"<p>Search data in Elasticsearch using the Scroll API if necessary.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>str</code> <p>The name of the Elasticsearch index to search.</p> <code>q</code> <code>str</code> <p>The query string to search.</p> <code>size</code> <code>int</code> <p>The number of results to return.</p> <p>Returns:</p> Name Type Description <code>response</code> <code>ObjectApiResponse[Any] | dict[str, Any]</code> <p>The response from the Elasticsearch search.</p> Source code in <code>src/CryptoFraudDetection/elasticsearch/data_retrieval.py</code> <pre><code>def search_data(\n    index: str,\n    q: str,\n    size: int | None = None,\n) -&gt; ObjectApiResponse[Any] | dict[str, Any]:\n    \"\"\"\n    Search data in Elasticsearch using the Scroll API if necessary.\n\n    Attributes:\n        index (str): The name of the Elasticsearch index to search.\n        q (str): The query string to search.\n        size (int): The number of results to return.\n\n    Returns:\n        response (ObjectApiResponse[Any] | dict[str, Any]): The response from the Elasticsearch search.\n\n    \"\"\"\n    if not size or size &lt;= 10000:\n        return es.search(index=index, q=q, size=size) if size else es.search(index=index, q=q, size=10000)\n\n    # use scroll API for large result sets\n    scroll = \"2m\"\n    batch_size = 1000\n    total_size = size\n\n    response = es.search(index=index, q=q, scroll=scroll, size=batch_size)\n\n    sid = response[\"_scroll_id\"]\n    hits = response[\"hits\"][\"hits\"]\n    all_hits = hits.copy()\n\n    while len(hits) &gt; 0 and len(all_hits) &lt; total_size:\n        response = es.scroll(scroll_id=sid, scroll=scroll)\n        sid = response[\"_scroll_id\"]\n        hits = response[\"hits\"][\"hits\"]\n        all_hits.extend(hits)\n\n        if len(all_hits) &gt;= total_size:\n            break\n\n    es.clear_scroll(scroll_id=sid)\n\n    all_hits = all_hits[:total_size]\n\n    return {\n        \"took\": response[\"took\"],\n        \"timed_out\": response[\"timed_out\"],\n        \"_shards\": response[\"_shards\"],\n        \"hits\": {\n            \"total\": {\"value\": len(all_hits), \"relation\": \"eq\"},\n            \"max_score\": None,\n            \"hits\": all_hits,\n        },\n    }\n</code></pre>"},{"location":"CryptoFraudDetection/elasticsearch/elastic_client/","title":"elastic_client","text":"<p>File: elastic_client.py.</p> Description <p>This file contains the Elasticsearch client.</p>"},{"location":"CryptoFraudDetection/elasticsearch/elastic_client/#src.CryptoFraudDetection.elasticsearch.elastic_client.get_elasticsearch_client","title":"<code>get_elasticsearch_client()</code>","text":"<p>Get the Elasticsearch client.</p> <p>Returns:</p> Name Type Description <code>es</code> <code>Elasticsearch</code> <p>The Elasticsearch client.</p> Source code in <code>src/CryptoFraudDetection/elasticsearch/elastic_client.py</code> <pre><code>def get_elasticsearch_client() -&gt; Elasticsearch:\n    \"\"\"\n    Get the Elasticsearch client.\n\n    Returns:\n        es (Elasticsearch): The Elasticsearch client.\n\n    \"\"\"\n    if ELASTICSEARCH_API_KEY in (\"changeme\", None):\n        raise APIKeyNotSetException\n\n    return Elasticsearch(\n        hosts=ELASTICSEARCH_HOSTNAME,\n        api_key=ELASTICSEARCH_API_KEY,\n        verify_certs=False,\n    )\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/comparitech/","title":"comparitech","text":"<p>File: comparitech.py.</p> Description <p>A web scraper for extracting cryptocurrency scam data from Comparitech's Crypto Scam List. Uses Selenium WebDriver to navigate and extract tabular data from the datawrapper interface. Implements specific error handling for each potential failure point.</p>"},{"location":"CryptoFraudDetection/scraper/comparitech/#src.CryptoFraudDetection.scraper.comparitech.ComparitechScraper","title":"<code>ComparitechScraper</code>","text":"<p>A scraper for extracting cryptocurrency scam data from Comparitech.</p> <p>This class handles the web scraping of tabular data from Comparitech's Crypto Scam List, including pagination and data extraction with specific error handling for each operation.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>Logger instance for tracking scraping progress and errors.</p> <code>base_url</code> <code>str</code> <p>Base URL for the Comparitech Crypto Scam List.</p> <code>page_load_timeout</code> <code>int</code> <p>Maximum time in seconds to wait for a page to load.</p> <code>element_wait_timeout</code> <code>int</code> <p>Maximum time in seconds to wait for an element to appear.</p> Example <p>logger = Logger(\"crypto_scraper\") scraper = ComparitechScraper(logger) data = scraper.get_data(headless=True)</p> Source code in <code>src/CryptoFraudDetection/scraper/comparitech.py</code> <pre><code>class ComparitechScraper:\n    \"\"\"\n    A scraper for extracting cryptocurrency scam data from Comparitech.\n\n    This class handles the web scraping of tabular data from Comparitech's Crypto Scam List,\n    including pagination and data extraction with specific error handling for each operation.\n\n    Attributes:\n        logger (Logger): Logger instance for tracking scraping progress and errors.\n        base_url (str): Base URL for the Comparitech Crypto Scam List.\n        page_load_timeout (int): Maximum time in seconds to wait for a page to load.\n        element_wait_timeout (int): Maximum time in seconds to wait for an element to appear.\n\n    Example:\n        &gt;&gt;&gt; logger = Logger(\"crypto_scraper\")\n        &gt;&gt;&gt; scraper = ComparitechScraper(logger)\n        &gt;&gt;&gt; data = scraper.get_data(headless=True)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        logger: Logger,\n        base_url: str = \"https://datawrapper.dwcdn.net/9nRA9/107/\",\n        page_load_timeout: int = 30,\n        element_wait_timeout: int = 10,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ComparitechScraper.\n\n        Args:\n            logger: Logger instance for tracking scraping progress and errors\n            base_url: Base URL for the Comparitech Crypto Scam List\n            page_load_timeout: Maximum time in seconds to wait for page loads\n            element_wait_timeout: Maximum time in seconds to wait for elements to appear\n\n        \"\"\"\n        self.logger = logger\n        self.base_url = base_url\n        self.page_load_timeout = page_load_timeout\n        self.element_wait_timeout = element_wait_timeout\n\n    def get_data(\n        self,\n        headless: bool = True,\n        test_run: bool = False,\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Execute the scraping process and return collected data.\n\n        Args:\n            headless: Whether to run browser in headless mode. Defaults to True.\n            test_run: Whether to limit scraping to a small number of pages. Defaults to False.\n\n        Returns:\n            Dictionary mapping column names to lists of values. Each key represents\n            a column header, and its value is a list of strings containing the\n            data from that column.\n\n        Raises:\n            WebDriverException: If there are issues with browser initialization or control\n            TimeoutException: If the page fails to load within the specified timeout\n\n        \"\"\"\n        driver = None\n        data: dict[str, list[str]] = {}\n        try:\n            # Initialize the WebDriver with appropriate settings\n            self.logger.info(\"Initializing web driver\")\n            driver = utils.get_driver(headless)\n            driver.set_page_load_timeout(self.page_load_timeout)\n\n            # Execute the main scraping process\n            data = self._perform_scrape(driver, test_run)\n\n        except WebDriverException as e:\n            self.logger.handle_exception(\n                WebDriverException,\n                f\"Failed to initialize or use WebDriver: {e!s}\",\n            )\n\n        # Close the WebDriver instance after scraping\n        finally:\n            if driver:\n                try:\n                    self.logger.debug(\"Closing web driver\")\n                    driver.quit()\n                except WebDriverException as e:\n                    self.logger.handle_exception(\n                        WebDriverException,\n                        f\"Error closing WebDriver: {e!s}\",\n                        \"warning\",\n                    )\n\n        return data\n\n    def _perform_scrape(\n        self,\n        driver: WebDriver,\n        test_run: bool,\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Perform the main scraping operation.\n\n        Navigates through all pages of the table, collecting data from each page\n        until no more pages are available.\n\n        Args:\n            driver: Selenium WebDriver instance to use for web interactions\n            test_run: Whether to limit scraping to a small number of pages\n\n        Returns:\n            Dictionary containing the scraped data, mapping column names to lists\n            of values from all pages combined\n\n        Raises:\n            TimeoutException: If page loading times out\n            WebDriverException: For other WebDriver related errors\n            StaleElementReferenceException: If page elements become stale during scraping\n\n        \"\"\"\n        try:\n            # Navigate to the target URL\n            self.logger.info(f\"Navigating to {self.base_url}\")\n            driver.get(self.base_url)\n\n        except TimeoutException as e:\n            self.logger.handle_exception(\n                TimeoutException,\n                f\"Failed to load the initial page - timeout: {e!s}\",\n            )\n\n        except WebDriverException as e:\n            self.logger.handle_exception(\n                WebDriverException,\n                f\"Failed to navigate to URL: {e!s}\",\n            )\n\n        # Initialize result storage and page counter\n        results: dict[str, list[str]] = defaultdict(list)\n        page_num = 0\n\n        while True:\n            page_num += 1\n\n            self.logger.info(f\"Scraping page {page_num}\")\n\n            # Extract data from current page\n            page_results = self._extract_results(driver)\n\n            # Break if no data found on current page\n            if not page_results:\n                self.logger.info(f\"No data found on page {page_num}\")\n                break\n\n            # Merge current page results into overall results\n            for key, value in page_results.items():\n                results[key].extend(value)\n\n            # Try to navigate to next page, break if no more pages\n            if not self._click_next_page(driver):\n                break\n\n            # Break early if test run\n            if test_run and page_num == 3:\n                break\n\n        self.logger.info(f\"Completed scraping {page_num} pages\")\n        return dict(results)\n\n    def _extract_results(self, driver: WebDriver) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Extract data from the current page's table.\n\n        Locates the table on the current page and extracts all cell values,\n        organizing them by column headers.\n\n        Args:\n            driver: Selenium WebDriver instance pointing to the page with the table\n\n        Returns:\n            Dictionary mapping column names to lists of values for the current page.\n            Returns empty dictionary if no data is found or on error.\n\n        Raises:\n            TimeoutException: If table elements don't load within timeout\n            StaleElementReferenceException: If elements become stale during processing\n\n        \"\"\"\n        results: dict[str, list[str]] = defaultdict(list)\n\n        try:\n            # Wait for table to become available in the DOM\n            self.logger.debug(\"Waiting for table to load\")\n            table = WebDriverWait(driver, self.element_wait_timeout).until(\n                EC.presence_of_element_located((By.TAG_NAME, \"table\")),\n            )\n\n            # Extract and validate table headers\n            columns = table.find_elements(By.TAG_NAME, \"th\")\n            if not columns:\n                self.logger.info(\"No table headers found\")\n                return results\n\n            # Extract and validate table rows\n            tbody = table.find_element(By.TAG_NAME, \"tbody\")\n            rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n            if not rows:\n                self.logger.info(\"No table rows found\")\n                return results\n\n            # Process each row and extract cell data\n            for row in rows:\n                th_cells = row.find_elements(By.TAG_NAME, \"th\")\n                td_cells = row.find_elements(By.TAG_NAME, \"td\")\n                cells = th_cells + td_cells\n\n                # Map cell data to corresponding column\n                for i, cell in enumerate(cells):\n                    if i &lt; len(columns):  # Ensure column index exists\n                        cell_text = cell.text.strip()\n                        results[columns[i].text].append(cell_text)\n\n        except TimeoutException as e:\n            self.logger.handle_exception(\n                TimeoutException,\n                f\"Timeout waiting for table to load: {e!s}\",\n            )\n\n        except StaleElementReferenceException as e:\n            self.logger.handle_exception(\n                StaleElementReferenceException,\n                f\"Elements became stale during extraction: {e!s}\",\n            )\n\n        return results\n\n    def _click_next_page(self, driver: WebDriver) -&gt; bool:\n        \"\"\"\n        Attempt to navigate to the next page of results.\n\n        Uses JavaScript to click the next page button, as this is more reliable\n        than selenium's click method for this particular interface.\n\n        Args:\n            driver: Selenium WebDriver instance on the current page\n\n        Returns:\n            True if successfully navigated to next page, False if at last page\n            or if navigation fails\n\n        \"\"\"\n        try:\n            # Attempt to click the next page button using JavaScript\n            self.logger.debug(\"Attempting to click next page button\")\n            driver.execute_script(\n                \"document.querySelector('button.next').click()\",\n            )\n            return True\n\n        except JavascriptException:\n            self.logger.debug(\"Reached last page or next button not found\")\n            return False\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/comparitech/#src.CryptoFraudDetection.scraper.comparitech.ComparitechScraper.__init__","title":"<code>__init__(logger, base_url='https://datawrapper.dwcdn.net/9nRA9/107/', page_load_timeout=30, element_wait_timeout=10)</code>","text":"<p>Initialize the ComparitechScraper.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>Logger instance for tracking scraping progress and errors</p> required <code>base_url</code> <code>str</code> <p>Base URL for the Comparitech Crypto Scam List</p> <code>'https://datawrapper.dwcdn.net/9nRA9/107/'</code> <code>page_load_timeout</code> <code>int</code> <p>Maximum time in seconds to wait for page loads</p> <code>30</code> <code>element_wait_timeout</code> <code>int</code> <p>Maximum time in seconds to wait for elements to appear</p> <code>10</code> Source code in <code>src/CryptoFraudDetection/scraper/comparitech.py</code> <pre><code>def __init__(\n    self,\n    logger: Logger,\n    base_url: str = \"https://datawrapper.dwcdn.net/9nRA9/107/\",\n    page_load_timeout: int = 30,\n    element_wait_timeout: int = 10,\n) -&gt; None:\n    \"\"\"\n    Initialize the ComparitechScraper.\n\n    Args:\n        logger: Logger instance for tracking scraping progress and errors\n        base_url: Base URL for the Comparitech Crypto Scam List\n        page_load_timeout: Maximum time in seconds to wait for page loads\n        element_wait_timeout: Maximum time in seconds to wait for elements to appear\n\n    \"\"\"\n    self.logger = logger\n    self.base_url = base_url\n    self.page_load_timeout = page_load_timeout\n    self.element_wait_timeout = element_wait_timeout\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/comparitech/#src.CryptoFraudDetection.scraper.comparitech.ComparitechScraper.get_data","title":"<code>get_data(headless=True, test_run=False)</code>","text":"<p>Execute the scraping process and return collected data.</p> <p>Parameters:</p> Name Type Description Default <code>headless</code> <code>bool</code> <p>Whether to run browser in headless mode. Defaults to True.</p> <code>True</code> <code>test_run</code> <code>bool</code> <p>Whether to limit scraping to a small number of pages. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary mapping column names to lists of values. Each key represents</p> <code>dict[str, list[str]]</code> <p>a column header, and its value is a list of strings containing the</p> <code>dict[str, list[str]]</code> <p>data from that column.</p> <p>Raises:</p> Type Description <code>WebDriverException</code> <p>If there are issues with browser initialization or control</p> <code>TimeoutException</code> <p>If the page fails to load within the specified timeout</p> Source code in <code>src/CryptoFraudDetection/scraper/comparitech.py</code> <pre><code>def get_data(\n    self,\n    headless: bool = True,\n    test_run: bool = False,\n) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Execute the scraping process and return collected data.\n\n    Args:\n        headless: Whether to run browser in headless mode. Defaults to True.\n        test_run: Whether to limit scraping to a small number of pages. Defaults to False.\n\n    Returns:\n        Dictionary mapping column names to lists of values. Each key represents\n        a column header, and its value is a list of strings containing the\n        data from that column.\n\n    Raises:\n        WebDriverException: If there are issues with browser initialization or control\n        TimeoutException: If the page fails to load within the specified timeout\n\n    \"\"\"\n    driver = None\n    data: dict[str, list[str]] = {}\n    try:\n        # Initialize the WebDriver with appropriate settings\n        self.logger.info(\"Initializing web driver\")\n        driver = utils.get_driver(headless)\n        driver.set_page_load_timeout(self.page_load_timeout)\n\n        # Execute the main scraping process\n        data = self._perform_scrape(driver, test_run)\n\n    except WebDriverException as e:\n        self.logger.handle_exception(\n            WebDriverException,\n            f\"Failed to initialize or use WebDriver: {e!s}\",\n        )\n\n    # Close the WebDriver instance after scraping\n    finally:\n        if driver:\n            try:\n                self.logger.debug(\"Closing web driver\")\n                driver.quit()\n            except WebDriverException as e:\n                self.logger.handle_exception(\n                    WebDriverException,\n                    f\"Error closing WebDriver: {e!s}\",\n                    \"warning\",\n                )\n\n    return data\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/google_results/","title":"google_results","text":"<p>File: google_results.py.</p> Description <p>This file contains the GoogleResultsScraper class, used to scrape search results from Google.</p>"},{"location":"CryptoFraudDetection/scraper/google_results/#src.CryptoFraudDetection.scraper.google_results.GoogleResultsScraper","title":"<code>GoogleResultsScraper</code>","text":"<p>A scraper class that interacts with Google Search to extract search result links, titles, and descriptions using Selenium.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>Logger instance used for logging.</p> <code>box_class</code> <code>str</code> <p>CSS class name for search result boxes.</p> <code>desc_class</code> <code>str</code> <p>CSS class name for search result descriptions.</p> <code>cookie_id</code> <code>str</code> <p>ID for Google's cookie acceptance button.</p> <code>search_box_id</code> <code>str</code> <p>ID for Google's search box.</p> <code>next_button_id</code> <code>str</code> <p>ID for Google's 'Next' button to go to the next page of results.</p> <p>Methods:</p> Name Description <code>get_main_results</code> <p>Scrapes Google search results for the given query.</p> <code>_validate_input</code> <p>Validates the number of search result pages to scrape.</p> <code>_perform_search</code> <p>Opens Google, accepts cookies, and submits the search query.</p> <code>_accept_cookies</code> <p>Accepts Google's cookies if the prompt appears.</p> <code>_submit_search_query</code> <p>Finds the search box, enters the query, and submits it.</p> <code>_scrape_multiple_pages</code> <p>Loops through multiple result pages and extracts search results.</p> <code>_get_result_boxes</code> <p>Fetches search result boxes from the current Google result page.</p> <code>_extract_results</code> <p>Extracts links, titles, and descriptions from result boxes.</p> <code>_click_next_page</code> <p>Clicks the 'Next' button to go to the next result page.</p> Source code in <code>src/CryptoFraudDetection/scraper/google_results.py</code> <pre><code>class GoogleResultsScraper:\n    \"\"\"\n    A scraper class that interacts with Google Search to extract search result links,\n    titles, and descriptions using Selenium.\n\n    Attributes:\n        logger (Logger): Logger instance used for logging.\n        box_class (str): CSS class name for search result boxes.\n        desc_class (str): CSS class name for search result descriptions.\n        cookie_id (str): ID for Google's cookie acceptance button.\n        search_box_id (str): ID for Google's search box.\n        next_button_id (str): ID for Google's 'Next' button to go to the next page of results.\n\n    Methods:\n        get_main_results: Scrapes Google search results for the given query.\n        _validate_input: Validates the number of search result pages to scrape.\n        _perform_search: Opens Google, accepts cookies, and submits the search query.\n        _accept_cookies: Accepts Google's cookies if the prompt appears.\n        _submit_search_query: Finds the search box, enters the query, and submits it.\n        _scrape_multiple_pages: Loops through multiple result pages and extracts search results.\n        _get_result_boxes: Fetches search result boxes from the current Google result page.\n        _extract_results: Extracts links, titles, and descriptions from result boxes.\n        _click_next_page: Clicks the 'Next' button to go to the next result page.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        logger: Logger,\n        box_class: str = \"MjjYud\",\n        desc_class: str = \"VwiC3b\",\n        cookie_id: str = \"L2AGLb\",\n        search_box_id: str = \"APjFqb\",\n        next_button_id: str = \"pnnext\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GoogleResultsScraper class.\n\n        Args:\n            logger (Logger): The logger instance to use for logging.\n            box_class (str): CSS class name for the search result container. Defaults to \"MjjYud\".\n            desc_class (str): CSS class name for the description. Defaults to \"VwiC3b\".\n            cookie_id (str): ID for the cookie acceptance button. Defaults to \"L2AGLb\".\n            search_box_id (str): ID for the search input box. Defaults to \"APjFqb\".\n            next_button_id (str): ID for the next page button. Defaults to \"pnnext\".\n\n        \"\"\"\n        self.logger = logger\n        self.box_class = box_class\n        self.desc_class = desc_class\n        self.cookie_id = cookie_id\n        self.search_box_id = search_box_id\n        self.next_button_id = next_button_id\n        self.query = \"\"\n\n    def get_main_results(\n        self,\n        query: str,\n        n_sites: int = 5,\n        delay_between_pages: float = 1.0,\n        headless: bool = False,\n        proxy_protocol: str | None = None,\n        proxy_address: str | None = None,\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Scrape Google search results for the given query.\n\n        Args:\n            query (str): The search query to be performed on Google.\n            n_sites (int): The number of Google search result pages to scrape (default is 5).\n            delay_between_pages (float): Delay (in seconds) between page navigations.\n                Default is 1 second.\n            headless (bool): Whether to run the browser in headless mode (default is False).\n            proxy_address (str): The proxy address to use for the browser (default is None).\n\n        Returns:\n            Dict[str, List[str]]: A dictionary containing the links, titles,\n            and descriptions of the search results.\n\n        \"\"\"\n        self.query = query\n        self._validate_input(n_sites)\n        driver = utils.get_driver(\n            headless=headless,\n            proxy_protocol=proxy_protocol,\n            proxy_address=proxy_address,\n        )\n\n        try:\n            self._perform_search(driver, self.query, delay_between_pages)\n            time.sleep(delay_between_pages)\n            results = self._scrape_multiple_pages(\n                driver,\n                n_sites,\n                delay_between_pages,\n            )\n        finally:\n            driver.quit()\n\n        return results\n\n    def _validate_input(self, n_sites: int) -&gt; None:\n        \"\"\"\n        Validate the number of search result pages to scrape.\n\n        Args:\n            n_sites (int): The number of Google search result pages to scrape.\n\n        Raises:\n            InvalidParameterException: If the number of sites is less than 1.\n\n        \"\"\"\n        if n_sites &lt; 1:\n            self.logger.handle_exception(\n                InvalidParameterException,\n                \"Number of sites must be at least 1\",\n            )\n\n    def _perform_search(\n        self,\n        driver: webdriver.Firefox,\n        query: str,\n        delay: float,\n    ) -&gt; None:\n        \"\"\"\n        Open Google, accept cookies, and submit the search query.\n\n        Args:\n            driver (WebDriver): The Selenium WebDriver instance.\n            query (str): The search query to be performed on Google.\n            delay (float): The delay between actions (in seconds).\n\n        \"\"\"\n        driver.get(\"https://www.google.com\")\n        self._accept_cookies(driver)\n        time.sleep(delay)\n        self._submit_search_query(driver, query)\n\n    def _accept_cookies(self, driver: webdriver.Firefox) -&gt; None:\n        \"\"\"\n        Accept Google's cookies if the prompt appears.\n\n        Args:\n            driver (WebDriver): The Selenium WebDriver instance.\n\n        \"\"\"\n        try:\n            driver.find_element(By.ID, self.cookie_id).click()\n            self.logger.info(\"Accepted Google's cookies.\")\n        except (NoSuchElementException, TimeoutException):\n            self.logger.warning(\"Cookie acceptance button not found.\")\n\n    def _submit_search_query(\n        self,\n        driver: webdriver.Firefox,\n        query: str,\n    ) -&gt; None:\n        \"\"\"\n        Find the search box, enter the query, and submit it.\n\n        Args:\n            driver (WebDriver): The Selenium WebDriver instance.\n            query (str): The search query to be submitted.\n\n        Raises:\n            NoSuchElementException: If the search box is not found on the page.\n\n        \"\"\"\n        try:\n            search_box = driver.find_element(By.ID, self.search_box_id)\n            search_box.send_keys(query)\n            search_box.submit()\n            self.logger.info(\"Search query submitted successfully.\")\n        except NoSuchElementException:\n            self.logger.handle_exception(\n                NoSuchElementException,\n                \"Could not find the search box element.\",\n            )\n\n    def _scrape_multiple_pages(\n        self,\n        driver: webdriver.Firefox,\n        n_sites: int,\n        delay: float,\n    ) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Loops through multiple result pages and extracts search results.\n\n        Args:\n            driver (WebDriver): The Selenium WebDriver instance.\n            n_sites (int): The number of Google search result pages to scrape.\n            delay (float): Delay (in seconds) between page navigations.\n\n        Returns:\n            Dict[str, List[str]]: A dictionary containing the links, titles,\n            and descriptions of the search results.\n\n        \"\"\"\n        results: dict[str, list[str]] = defaultdict(list)\n\n        for i in range(n_sites):\n            time.sleep(delay)\n            result_boxes = self._get_result_boxes(driver)\n            if not result_boxes:\n                self.logger.handle_exception(\n                    DetectedBotException,\n                    \"No results found. Possibly blocked by Google.\",\n                )\n\n            self._extract_results(result_boxes, results)\n\n            if i != n_sites - 1 and not self._click_next_page(driver):\n                break\n\n        return results\n\n    def _get_result_boxes(self, driver: webdriver.Firefox) -&gt; list[WebElement]:\n        \"\"\"\n        Fetch search result boxes from the current Google result page.\n\n        Args:\n            driver (WebDriver): The Selenium WebDriver instance.\n\n        Returns:\n            List[WebElement]: A list of WebElement objects representing search result boxes.\n\n        Raises:\n            NoSuchElementException: If no result boxes are found on the page.\n\n        \"\"\"\n        try:\n            return driver.find_elements(By.CLASS_NAME, self.box_class)\n        except NoSuchElementException:\n            self.logger.handle_exception(\n                NoSuchElementException,\n                \"Could not find any result boxes.\",\n            )\n            return []\n\n    def _extract_results(\n        self,\n        result_boxes: list[WebElement],\n        results: dict[str, list[str]],\n    ) -&gt; None:\n        \"\"\"\n        Extract links, titles, and descriptions from result boxes.\n\n        Args:\n            result_boxes (List[WebElement]): A list of WebElement objects\n                representing search result boxes.\n            results (Dict[str, List[str]]): A dictionary to store extracted\n                links, titles, and descriptions.\n\n        \"\"\"\n        for box in result_boxes:\n            try:\n                link = str(\n                    box.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\"),\n                )\n                title = str(box.find_element(By.TAG_NAME, \"h3\").text)\n                desc = str(\n                    box.find_element(By.CLASS_NAME, self.desc_class).text,\n                )\n\n                results[\"link\"].append(link)\n                results[\"title\"].append(title)\n                results[\"description\"].append(desc)\n                results[\"query\"].append(self.query)\n\n                id_pre_hash = link + title\n                results[\"id\"].append(\n                    hashlib.md5(id_pre_hash.encode()).hexdigest(),\n                )\n            except NoSuchElementException:\n                self.logger.debug(\"Missing elements in result box. Skipping\")\n                continue\n\n    def _click_next_page(self, driver: webdriver.Firefox) -&gt; bool:\n        \"\"\"\n        Clicks the 'Next' button to go to the next result page.\n\n        Args:\n            driver (WebDriver): The Selenium WebDriver instance.\n\n        Returns:\n            bool: True if the 'Next' button was clicked successfully, False if not.\n\n        Raises:\n            NoSuchElementException: If the next page button is not found on the page.\n\n        \"\"\"\n        try:\n            next_button = driver.find_element(By.ID, self.next_button_id)\n            next_button.click()\n            self.logger.info(\"Navigated to the next page of results.\")\n            return True\n        except NoSuchElementException:\n            self.logger.warning(\"Next page button not found.\")\n            return False\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/google_results/#src.CryptoFraudDetection.scraper.google_results.GoogleResultsScraper.__init__","title":"<code>__init__(logger, box_class='MjjYud', desc_class='VwiC3b', cookie_id='L2AGLb', search_box_id='APjFqb', next_button_id='pnnext')</code>","text":"<p>Initialize the GoogleResultsScraper class.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The logger instance to use for logging.</p> required <code>box_class</code> <code>str</code> <p>CSS class name for the search result container. Defaults to \"MjjYud\".</p> <code>'MjjYud'</code> <code>desc_class</code> <code>str</code> <p>CSS class name for the description. Defaults to \"VwiC3b\".</p> <code>'VwiC3b'</code> <code>cookie_id</code> <code>str</code> <p>ID for the cookie acceptance button. Defaults to \"L2AGLb\".</p> <code>'L2AGLb'</code> <code>search_box_id</code> <code>str</code> <p>ID for the search input box. Defaults to \"APjFqb\".</p> <code>'APjFqb'</code> <code>next_button_id</code> <code>str</code> <p>ID for the next page button. Defaults to \"pnnext\".</p> <code>'pnnext'</code> Source code in <code>src/CryptoFraudDetection/scraper/google_results.py</code> <pre><code>def __init__(\n    self,\n    logger: Logger,\n    box_class: str = \"MjjYud\",\n    desc_class: str = \"VwiC3b\",\n    cookie_id: str = \"L2AGLb\",\n    search_box_id: str = \"APjFqb\",\n    next_button_id: str = \"pnnext\",\n) -&gt; None:\n    \"\"\"\n    Initialize the GoogleResultsScraper class.\n\n    Args:\n        logger (Logger): The logger instance to use for logging.\n        box_class (str): CSS class name for the search result container. Defaults to \"MjjYud\".\n        desc_class (str): CSS class name for the description. Defaults to \"VwiC3b\".\n        cookie_id (str): ID for the cookie acceptance button. Defaults to \"L2AGLb\".\n        search_box_id (str): ID for the search input box. Defaults to \"APjFqb\".\n        next_button_id (str): ID for the next page button. Defaults to \"pnnext\".\n\n    \"\"\"\n    self.logger = logger\n    self.box_class = box_class\n    self.desc_class = desc_class\n    self.cookie_id = cookie_id\n    self.search_box_id = search_box_id\n    self.next_button_id = next_button_id\n    self.query = \"\"\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/google_results/#src.CryptoFraudDetection.scraper.google_results.GoogleResultsScraper.get_main_results","title":"<code>get_main_results(query, n_sites=5, delay_between_pages=1.0, headless=False, proxy_protocol=None, proxy_address=None)</code>","text":"<p>Scrape Google search results for the given query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to be performed on Google.</p> required <code>n_sites</code> <code>int</code> <p>The number of Google search result pages to scrape (default is 5).</p> <code>5</code> <code>delay_between_pages</code> <code>float</code> <p>Delay (in seconds) between page navigations. Default is 1 second.</p> <code>1.0</code> <code>headless</code> <code>bool</code> <p>Whether to run the browser in headless mode (default is False).</p> <code>False</code> <code>proxy_address</code> <code>str</code> <p>The proxy address to use for the browser (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dict[str, List[str]]: A dictionary containing the links, titles,</p> <code>dict[str, list[str]]</code> <p>and descriptions of the search results.</p> Source code in <code>src/CryptoFraudDetection/scraper/google_results.py</code> <pre><code>def get_main_results(\n    self,\n    query: str,\n    n_sites: int = 5,\n    delay_between_pages: float = 1.0,\n    headless: bool = False,\n    proxy_protocol: str | None = None,\n    proxy_address: str | None = None,\n) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Scrape Google search results for the given query.\n\n    Args:\n        query (str): The search query to be performed on Google.\n        n_sites (int): The number of Google search result pages to scrape (default is 5).\n        delay_between_pages (float): Delay (in seconds) between page navigations.\n            Default is 1 second.\n        headless (bool): Whether to run the browser in headless mode (default is False).\n        proxy_address (str): The proxy address to use for the browser (default is None).\n\n    Returns:\n        Dict[str, List[str]]: A dictionary containing the links, titles,\n        and descriptions of the search results.\n\n    \"\"\"\n    self.query = query\n    self._validate_input(n_sites)\n    driver = utils.get_driver(\n        headless=headless,\n        proxy_protocol=proxy_protocol,\n        proxy_address=proxy_address,\n    )\n\n    try:\n        self._perform_search(driver, self.query, delay_between_pages)\n        time.sleep(delay_between_pages)\n        results = self._scrape_multiple_pages(\n            driver,\n            n_sites,\n            delay_between_pages,\n        )\n    finally:\n        driver.quit()\n\n    return results\n</code></pre>"},{"location":"CryptoFraudDetection/scraper/utils/","title":"utils","text":"<p>File: utils.py.</p> Description <p>These functions are not specific to any particular part of the codebase, and are used in multiple places throughout the project.</p>"},{"location":"CryptoFraudDetection/scraper/utils/#src.CryptoFraudDetection.scraper.utils.get_driver","title":"<code>get_driver(headless=False, proxy_protocol=None, proxy_address=None)</code>","text":"<p>Return a Selenium Chrome WebDriver object.</p> <p>Parameters:</p> Name Type Description Default <code>headless</code> <code>bool</code> <p>Whether to run the browser in headless mode.</p> <code>False</code> <code>proxy_protocol</code> <code>str</code> <p>The protocol of the proxy to use.</p> <code>None</code> <code>proxy_address</code> <code>str</code> <p>The address of the proxy to use. : <code>None</code> <p>Returns:</p> Name Type Description <code>WebDriver</code> <code>Firefox</code> <p>A Selenium WebDriver object.</p> Source code in <code>src/CryptoFraudDetection/scraper/utils.py</code> <pre><code>def get_driver(\n    headless: bool = False,\n    proxy_protocol: str | None = None,\n    proxy_address: str | None = None,\n) -&gt; webdriver.Firefox:\n    \"\"\"\n    Return a Selenium Chrome WebDriver object.\n\n    Args:\n        headless (bool): Whether to run the browser in headless mode.\n        proxy_protocol (str): The protocol of the proxy to use.\n        proxy_address (str): The address of the proxy to use. &lt;ip&gt;:&lt;port&gt;\n\n    Returns:\n        WebDriver: A Selenium WebDriver object.\n\n    \"\"\"\n    driver = None\n\n    options = webdriver.FirefoxOptions()\n    options.set_preference(\"devtools.jsonview.enabled\", False)\n\n    if headless:\n        options.add_argument(\"--headless\")\n\n    if proxy_protocol and proxy_address:\n        proxy = Proxy(\n            {\n                \"proxyType\": ProxyType.MANUAL,\n            },\n        )\n\n        match proxy_protocol:\n            case \"http\":\n                proxy.http_proxy = proxy_address\n                proxy.ssl_proxy = proxy_address\n            case \"socks4\":\n                proxy.socks_proxy = proxy_address\n                proxy.socks_version = 4\n            case \"socks5\":\n                proxy.socks_proxy = proxy_address\n                proxy.socks_version = 5\n            case _:\n                msg = f\"Proxy protocol {proxy_protocol} is not implemented.\"\n                raise ProxyProtocolNotImplemented(\n                    msg,\n                )\n\n        options.proxy = proxy\n\n        driver = webdriver.Firefox(options=options)\n        try:\n            driver.set_page_load_timeout(15)\n            driver.get(\"https://httpbin.io/ip\")\n\n            fetched_element = driver.find_element(\"tag name\", \"pre\").text\n            fetched_json = json.loads(fetched_element)\n            fetched_ip = fetched_json[\"origin\"].split(\":\")[0]\n\n            real_ip = requests.get(\"https://api.ipify.org\", timeout=10).text\n\n            if fetched_ip == real_ip:\n                msg = f\"Proxy IP {fetched_ip} is equal to the real IP {real_ip}. The proxy is not working.\"\n                raise ProxyIpEqualRealIp(\n                    msg,\n                )\n        except (\n            ProxyIpEqualRealIp,\n            WebDriverException,\n            NoSuchElementException,\n        ) as e:\n            driver.quit()\n            msg = f\"Proxy {proxy_protocol}:{proxy_address} is not working.\"\n            raise ProxyNotWorking(\n                msg,\n            ) from e\n\n    driver = driver if driver else webdriver.Firefox(options=options)\n    driver.set_page_load_timeout(60)\n\n    return driver\n</code></pre>"},{"location":"CryptoFraudDetection/utils/enums/","title":"enums","text":"<p>File: enums.py.</p> Description <p>This file contains the enums used in the project.</p>"},{"location":"CryptoFraudDetection/utils/enums/#src.CryptoFraudDetection.utils.enums.LoggerMode","title":"<code>LoggerMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum recreating for the logging package.</p> Source code in <code>src/CryptoFraudDetection/utils/enums.py</code> <pre><code>class LoggerMode(Enum):\n    \"\"\"Enum recreating for the logging package.\"\"\"\n\n    DEBUG = 10\n    INFO = 20\n    WARNING = 30\n    ERROR = 40\n    FATAL = 50\n</code></pre>"},{"location":"CryptoFraudDetection/utils/enums/#src.CryptoFraudDetection.utils.enums.ScraperNotebookMode","title":"<code>ScraperNotebookMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for the notebook mode.</p> Source code in <code>src/CryptoFraudDetection/utils/enums.py</code> <pre><code>class ScraperNotebookMode(Enum):\n    \"\"\"Enum for the notebook mode.\"\"\"\n\n    WRITE = \"write\"\n    READ = \"read\"\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/","title":"exceptions","text":"<p>File: exceptions.py.</p> Description <p>This file contains the exceptions used in the project.</p>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.APIKeyNotSetException","title":"<code>APIKeyNotSetException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the API key is not set.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class APIKeyNotSetException(Exception):\n    \"\"\"Exception raised when the API key is not set.\"\"\"\n\n    def __init__(\n        self,\n        message=\"API Key not set, please set it in the .env file and load it using 'source .env'\",\n    ) -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.AuthenticationError","title":"<code>AuthenticationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when authentication details are missing for scraping Twitter.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class AuthenticationError(Exception):\n    \"\"\"Exception raised when authentication details are missing for scraping Twitter.\"\"\"\n\n    def __init__(\n        self,\n        message=\"Authentication details are required (either cookies or username/password).\",\n    ) -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.DetectedBotException","title":"<code>DetectedBotException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a bot is detected.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class DetectedBotException(Exception):\n    \"\"\"Exception raised when a bot is detected.\"\"\"\n\n    def __init__(self, message=\"Detected as a bot\") -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.InvalidParameterException","title":"<code>InvalidParameterException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an invalid parameter is passed.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class InvalidParameterException(Exception):\n    \"\"\"Exception raised when an invalid parameter is passed.\"\"\"\n\n    def __init__(self, message=\"Invalid parameter\") -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.ProxyIpEqualRealIp","title":"<code>ProxyIpEqualRealIp</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the proxy IP is equal to the real IP.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class ProxyIpEqualRealIp(Exception):\n    \"\"\"Exception raised when the proxy IP is equal to the real IP.\"\"\"\n\n    def __init__(self, message=\"Proxy IP is equal to the real IP.\") -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.ProxyNotWorking","title":"<code>ProxyNotWorking</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the proxy is not working.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class ProxyNotWorking(Exception):\n    \"\"\"Exception raised when the proxy is not working.\"\"\"\n\n    def __init__(self, message=\"Proxy is not working.\") -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/exceptions/#src.CryptoFraudDetection.utils.exceptions.ProxyProtocolNotImplemented","title":"<code>ProxyProtocolNotImplemented</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the proxy protocol is not implemented.</p> Source code in <code>src/CryptoFraudDetection/utils/exceptions.py</code> <pre><code>class ProxyProtocolNotImplemented(Exception):\n    \"\"\"Exception raised when the proxy protocol is not implemented.\"\"\"\n\n    def __init__(self, message=\"Proxy protocol is not implemented.\") -&gt; None:\n        self.message = message\n        super().__init__(self.message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/","title":"logger","text":"<p>File: logger.py.</p> Description <p>This file contains the Logger class used for logging messages to the console and a log file.</p>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger","title":"<code>Logger</code>","text":"<p>Logger class to handle logging messages to both the console and a file.</p> <p>Attributes:</p> Name Type Description <code>logger</code> <code>Logger</code> <p>The logger instance used for logging messages.</p> <p>Methods:</p> Name Description <code>info</code> <p>str) -&gt; None: Logs an informational message.</p> <code>warning</code> <p>str) -&gt; None: Logs a warning message.</p> <code>error</code> <p>str) -&gt; None: Logs an error message.</p> <code>critical</code> <p>str) -&gt; None: Logs a critical message.</p> Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>class Logger:\n    \"\"\"\n    Logger class to handle logging messages to both the console and a file.\n\n    Attributes:\n        logger (logging.Logger): The logger instance used for logging messages.\n\n    Methods:\n        info(message: str) -&gt; None: Logs an informational message.\n        warning(message: str) -&gt; None: Logs a warning message.\n        error(message: str) -&gt; None: Logs an error message.\n        critical(message: str) -&gt; None: Logs a critical message.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        level: LoggerMode = LoggerMode.DEBUG,\n        log_dir: str = \"logs\",\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Logger instance with the specified name, log level, and log directory.\n\n        Args:\n            name (str): The name of the logger, typically the name of the module or class.\n            level (int, optional): The log level (default: LoggerMode.DEBUG).\n            log_dir (str, optional): The directory where log files will be saved (default: \"logs\").\n\n        \"\"\"\n        self.logger: logging.Logger = logging.getLogger(name)\n        self.logger.setLevel(level.value)\n\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        )\n\n        # Set up console handler\n        stream_handler: logging.StreamHandler = logging.StreamHandler()\n        stream_handler.setFormatter(formatter)\n\n        # Add handlers only if there are no existing handlers (to avoid duplication)\n        if not self.logger.hasHandlers():\n            self.logger.addHandler(stream_handler)\n\n            # Create logs directory if it doesn't exist\n            if not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n\n            # Set up rotating file handler to handle log rotation and avoid large log files\n            time: str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n            file_handler: RotatingFileHandler = RotatingFileHandler(\n                f\"{log_dir}/{time}_{name}.log\",\n                maxBytes=1024 * 1024 * 5,\n                backupCount=5,\n            )\n            file_handler.setFormatter(formatter)\n            self.logger.addHandler(file_handler)\n\n    def debug(self, message: str) -&gt; None:\n        \"\"\"\n        Logs a debug message.\n\n        Args:\n            message (str): The message to log.\n\n        \"\"\"\n        self.logger.debug(message)\n\n    def info(self, message: str) -&gt; None:\n        \"\"\"\n        Logs an informational message.\n\n        Args:\n            message (str): The message to log.\n\n        \"\"\"\n        self.logger.info(message)\n\n    def warning(self, message: str) -&gt; None:\n        \"\"\"\n        Logs a warning message.\n\n        Args:\n            message (str): The message to log.\n\n        \"\"\"\n        self.logger.warning(message)\n\n    def error(self, message: str) -&gt; None:\n        \"\"\"\n        Logs an error message.\n\n        Args:\n            message (str): The message to log.\n\n        \"\"\"\n        self.logger.error(message)\n\n    def critical(self, message: str) -&gt; None:\n        \"\"\"\n        Logs a critical message.\n\n        Args:\n            message (str): The message to log.\n\n        \"\"\"\n        self.logger.critical(message)\n\n    def handle_exception(\n        self,\n        exception_class: type[Exception],\n        message: str,\n        logger_level: str = \"error\",\n    ) -&gt; None:\n        \"\"\"\n        Handles exception logging and raising.\n\n        Args:\n            exception_class (Exception): The class of the exception to raise.\n            message (str): The error message to log and raise.\n            logger_level (str): The logging level to use (\"error\", \"warning\", \"info\").\n                Defaults to \"error\".\n\n        \"\"\"\n        if logger_level == \"error\":\n            self.logger.error(message)\n            raise exception_class(message)\n        if logger_level == \"warning\":\n            self.logger.warning(message)\n        elif logger_level == \"info\":\n            self.logger.info(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.__init__","title":"<code>__init__(name, level=LoggerMode.DEBUG, log_dir='logs')</code>","text":"<p>Initializes the Logger instance with the specified name, log level, and log directory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the logger, typically the name of the module or class.</p> required <code>level</code> <code>int</code> <p>The log level (default: LoggerMode.DEBUG).</p> <code>DEBUG</code> <code>log_dir</code> <code>str</code> <p>The directory where log files will be saved (default: \"logs\").</p> <code>'logs'</code> Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    level: LoggerMode = LoggerMode.DEBUG,\n    log_dir: str = \"logs\",\n) -&gt; None:\n    \"\"\"\n    Initializes the Logger instance with the specified name, log level, and log directory.\n\n    Args:\n        name (str): The name of the logger, typically the name of the module or class.\n        level (int, optional): The log level (default: LoggerMode.DEBUG).\n        log_dir (str, optional): The directory where log files will be saved (default: \"logs\").\n\n    \"\"\"\n    self.logger: logging.Logger = logging.getLogger(name)\n    self.logger.setLevel(level.value)\n\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    )\n\n    # Set up console handler\n    stream_handler: logging.StreamHandler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n\n    # Add handlers only if there are no existing handlers (to avoid duplication)\n    if not self.logger.hasHandlers():\n        self.logger.addHandler(stream_handler)\n\n        # Create logs directory if it doesn't exist\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n\n        # Set up rotating file handler to handle log rotation and avoid large log files\n        time: str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        file_handler: RotatingFileHandler = RotatingFileHandler(\n            f\"{log_dir}/{time}_{name}.log\",\n            maxBytes=1024 * 1024 * 5,\n            backupCount=5,\n        )\n        file_handler.setFormatter(formatter)\n        self.logger.addHandler(file_handler)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.critical","title":"<code>critical(message)</code>","text":"<p>Logs a critical message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def critical(self, message: str) -&gt; None:\n    \"\"\"\n    Logs a critical message.\n\n    Args:\n        message (str): The message to log.\n\n    \"\"\"\n    self.logger.critical(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.debug","title":"<code>debug(message)</code>","text":"<p>Logs a debug message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def debug(self, message: str) -&gt; None:\n    \"\"\"\n    Logs a debug message.\n\n    Args:\n        message (str): The message to log.\n\n    \"\"\"\n    self.logger.debug(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.error","title":"<code>error(message)</code>","text":"<p>Logs an error message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def error(self, message: str) -&gt; None:\n    \"\"\"\n    Logs an error message.\n\n    Args:\n        message (str): The message to log.\n\n    \"\"\"\n    self.logger.error(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.handle_exception","title":"<code>handle_exception(exception_class, message, logger_level='error')</code>","text":"<p>Handles exception logging and raising.</p> <p>Parameters:</p> Name Type Description Default <code>exception_class</code> <code>Exception</code> <p>The class of the exception to raise.</p> required <code>message</code> <code>str</code> <p>The error message to log and raise.</p> required <code>logger_level</code> <code>str</code> <p>The logging level to use (\"error\", \"warning\", \"info\"). Defaults to \"error\".</p> <code>'error'</code> Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def handle_exception(\n    self,\n    exception_class: type[Exception],\n    message: str,\n    logger_level: str = \"error\",\n) -&gt; None:\n    \"\"\"\n    Handles exception logging and raising.\n\n    Args:\n        exception_class (Exception): The class of the exception to raise.\n        message (str): The error message to log and raise.\n        logger_level (str): The logging level to use (\"error\", \"warning\", \"info\").\n            Defaults to \"error\".\n\n    \"\"\"\n    if logger_level == \"error\":\n        self.logger.error(message)\n        raise exception_class(message)\n    if logger_level == \"warning\":\n        self.logger.warning(message)\n    elif logger_level == \"info\":\n        self.logger.info(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.info","title":"<code>info(message)</code>","text":"<p>Logs an informational message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def info(self, message: str) -&gt; None:\n    \"\"\"\n    Logs an informational message.\n\n    Args:\n        message (str): The message to log.\n\n    \"\"\"\n    self.logger.info(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/logger/#src.CryptoFraudDetection.utils.logger.Logger.warning","title":"<code>warning(message)</code>","text":"<p>Logs a warning message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to log.</p> required Source code in <code>src/CryptoFraudDetection/utils/logger.py</code> <pre><code>def warning(self, message: str) -&gt; None:\n    \"\"\"\n    Logs a warning message.\n\n    Args:\n        message (str): The message to log.\n\n    \"\"\"\n    self.logger.warning(message)\n</code></pre>"},{"location":"CryptoFraudDetection/utils/misc/","title":"misc","text":"<p>File: misc.py.</p> Description <p>These functions are not specific to any particular part of the codebase, and are used in multiple places throughout the project.</p>"},{"location":"CryptoFraudDetection/utils/misc/#src.CryptoFraudDetection.utils.misc.get_hello_world","title":"<code>get_hello_world()</code>","text":"<p>Return a simple hello world message.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing the hello world message.</p> Source code in <code>src/CryptoFraudDetection/utils/misc.py</code> <pre><code>def get_hello_world() -&gt; str:\n    \"\"\"\n    Return a simple hello world message.\n\n    Returns:\n        str: A string containing the hello world message.\n\n    \"\"\"\n    return \"Hello, World!\"\n</code></pre>"}]}