---
title: Scrape Reddit Posts
jupyter: python3
---

```{python}
from CryptoFraudDetection.utils.enums import ScraperNotebookMode, LoggerMode
from CryptoFraudDetection.utils.logger import Logger
from CryptoFraudDetection.scraper.reddit import RedditScraper
from CryptoFraudDetection.elasticsearch.data_insertion import insert_dataframe

import pandas as pd

logger = Logger(name="scrape_reddit", level=LoggerMode.WARNING, log_dir="../logs")
```

```{python}
# Enable or disable scraping
MODE = ScraperNotebookMode.WRITE
# General waiting time in second between requests
WAIT_RANGE = (0.125, 0.25)
# Number of subreddits and queries to scrape
# set to None to scrape all posts
LIMIT = None
```

```{python}
subreddits: list[str] = [
    "r/CryptoCurrency",
    "r/CryptoMoonShots",
    "r/CryptoMarkets",
    "r/Crypto",
    "r/Ethereum",
    "r/Bitcoin",
    "r/btc",
    "r/litecoin",
    "r/ethtrader",
    "r/Ripple",
    "r/BitcoinMarkets",
    "r/altcoin",
    "r/binance",
]

coins: list[str] = [
    "Bitcoin",
    "BTC",
    "Ethereum",
    "ETH",
    "Cosmos",
    "ATOM",
    "Avalanche",
    "AVAX",
    "FTX Token",
    "FTT",
    "Terra Luna Classic",
    "Terra Luna",
    "Terra Classic",
    "LUNC",
    "Squid-Game-Coin",
    "SQUID",
    "BeerCoin",
    "BEER",
    "BitForex",
    "BF",
    "BeerCoin",
    "BEER",
    "Safe Moon",
    "SAFEMOON",
    "Teddy Doge",
    "TEDDY V2",
    "STOA Network",
    "STA",
    "Chainlink",
    "LINK",
    "Polkadot",
    "DOT",
    "Solana",
    "SOL",
    "THORChain",
    "RUNE",
    "Avalanche",
    "AVAX",
    "Algorand",
    "ALGO",
    "Polygon",
    "MATIC",
    "Cardano",
    "ADA",
    "VeChain",
    "VET",
]
```

Testing until reddit blocks:

```{python}
subreddits_ = subreddits[:LIMIT] if LIMIT else subreddits
coins_ = coins[:LIMIT] if LIMIT else coins
```

```{python}
posts = {}
if MODE == ScraperNotebookMode.WRITE:
    for sub in subreddits_:
        for coin in coins_:
            try:  # if file already exists, skip scraping
                data: pd.DataFrame = pd.read_parquet(f"../data/reddit/{sub}_{coin}.parquet")
                print(f"Skipping scraping for sub: {sub}, coin: {coin} as is could be read from file.")
            except:  # scrape and save to file
                scraper = RedditScraper(
                    logger,
                    wait_range=WAIT_RANGE,
                    headless=False,
                )
                data: pd.DataFrame = scraper.scrape(sub, coin)
                data.to_parquet(f"../data/reddit/{sub}_{coin}.parquet")

            posts[(sub, coin)] = data
            print(f"sub: {sub}, coin: {coin}, len data: {len(data)}")
```


