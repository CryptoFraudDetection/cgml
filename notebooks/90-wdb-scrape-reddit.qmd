---
title: Scrape Reddit Posts
jupyter: python3
---
```{python}
from contextlib import contextmanager
import json
import os
from datetime import datetime
import time

import pandas as pd
import matplotlib.pyplot as plt


from CryptoFraudDetection.utils.enums import ScraperNotebookMode, LoggerMode
from CryptoFraudDetection.utils.logger import Logger
from CryptoFraudDetection.scraper.reddit import RedditScraper
from CryptoFraudDetection.elasticsearch.data_insertion import insert_dataframe
from CryptoFraudDetection.elasticsearch.data_retrieval import search_data

from CryptoFraudDetection import sentiment


logger = Logger(name="scrape_reddit", level=LoggerMode.INFO, log_dir="../logs")
```

```{python}
# Enable or disable scraping
MODE = ScraperNotebookMode.READ
# Number of subreddits and queries to scrape
# set to None to scrape all
SUBREDDIT_LIMIT = None
QUERY_LIMIT = None
DB_RETRY = 10
```

```{python}
subreddits: list[str] = [
    "r/CryptoCurrency",
    "r/CryptoMoonShots",
    "r/CryptoMarkets",
    "r/Crypto",
    "r/Ethereum",
    "r/Bitcoin",
    "r/btc",
    "r/litecoin",
    "r/ethtrader",
    "r/Ripple",
    "r/BitcoinMarkets",
    "r/altcoin",
    "r/binance",
]

coins: list[str] = [
    "Bitcoin",
    "BTC",
    "Ethereum",
    "ETH",
    "Cosmos",
    "ATOM",
    "Avalanche",
    "AVAX",
    "FTX Token",
    "FTT",
    "Terra Luna Classic",
    "Terra Luna",
    "Terra Classic",
    "LUNC",
    "Squid-Game-Coin",
    "SQUID",
    "BeerCoin",
    "BEER",
    "BitForex",
    "BF",
    "BeerCoin",
    "BEER",
    "Safe Moon",
    "SAFEMOON",
    "Teddy Doge",
    "TEDDY V2",
    "STOA Network",
    "STA",
    "Chainlink",
    "LINK",
    "Polkadot",
    "DOT",
    "Solana",
    "SOL",
    "THORChain",
    "RUNE",
    "Avalanche",
    "AVAX",
    "Algorand",
    "ALGO",
    "Polygon",
    "MATIC",
    "Cardano",
    "ADA",
    "VeChain",
    "VET",
]
```

```{python}
subreddits_ = subreddits[:SUBREDDIT_LIMIT] if SUBREDDIT_LIMIT else subreddits
coins_ = coins[:QUERY_LIMIT] if QUERY_LIMIT else coins
```

```{python}
class LockFileExistsError(Exception):
    """Custom exception raised when the lock file already exists."""
    pass

@contextmanager
def acquire_lock(lock_file_path):
    if os.path.exists(lock_file_path):
        raise LockFileExistsError(f"Lock file already exists: {lock_file_path}")
    try:
        with open(lock_file_path, "w") as lock_file:
            pass
        yield
    finally:
        if os.path.exists(lock_file_path):
            os.remove(lock_file_path)
```

```{python}
def scrape_subreddit_coin(subreddit:str, coin:str):
    # JSON file path
    sub_: str = subreddit.split("/")[-1].lower()
    coin_: str = coin.lower()
    json_path: str = f"../data/reddit/{sub_}_{coin_}.json"
    
    try:  # try-except block to handle lock file
        # Lock file for subreddit and coin combination
        # This is to prevent multiple processes from scraping the same subreddit and coin
        lock_file_path = json_path + '.lock'
        with acquire_lock(lock_file_path):
            # Skip scraping if file already exists
            data: list[dict] | None = None
            try:
                data = json.load(open(json_path))
                logger.info(
                    f"Skipping scraping for sub: {subreddit}, coin: {coin} as it could be read from file."
                )
            except FileNotFoundError:
                pass
            
            # Scrape if data is not read from file
            if data is None:
                # Scrape
                logger.info(f"Scraping for sub: {subreddit}, coin: {coin}")
                scraper = RedditScraper(logger, headless=True)
                scraper.scrape(subreddit, coin)
                data = scraper.post_data
                
                # Write to file
                with open(json_path, "w") as f:
                    f.write(json.dumps(data, indent=4))
            
            # convert to dataframe
            post_df = pd.DataFrame(data)
            post_df.rename(columns={"num_comment": "num_comments", "text": "body"}, inplace=True)
            post_df["subreddit"] = sub_
            post_df["search_query"] = coin_
            
            # Write to db
            for i in range(DB_RETRY):
                try:
                    if i > 0:
                        time.sleep(i**2)  # exponential backoff
                        logger.warning(f"Retry {i} for writing subreddit '{subreddit}' coin '{coin}' to db.")
                    insert_dataframe(logger, "wdb", post_df)
                    break
                except Exception as e:
                    logger.warning(f"Error occurred while writing to db. Error: {e}")

    # Skip if lock file is already acquired by another process
    except LockFileExistsError:
        logger.info(
            f"Skipping scraping for sub: {subreddit}, coin: {coin} as it is being scraped by another process."
        )
```

```{python}
if MODE == ScraperNotebookMode.WRITE:
    for subreddit in subreddits_:
        for coin in coins_:
            scrape_subreddit_coin(subreddit, coin)
```

## EDA 
```{python}
# edit subredditlist
subreddits = [subreddit.replace("r/", "").lower() for subreddit in subreddits]
#build query for elasticsearch
elasticquery_subreddits = [f"subreddit:{subreddit}" for subreddit in subreddits]
```
```{python}
#get count of data per subreddit
scraped_subreddits_posts = []

for subquery in elasticquery_subreddits:
    response = search_data(index="wdb", q=subquery)
    try:
        total_hits = response['hits']['total']['value']
        print(f"Anzahl der Einträge für {subquery}: {total_hits}")
    except Exception as e:
        print(f'Keine Ergebnisse für die Abfrage: {subquery}')
        continue

    scraped_subreddits_posts.append([subquery,total_hits])


```
```{python}
def visualize_subreddit_counts(scraped_subreddits_posts):
    '''
    This function visualizes the counts of sraped posts per subreddit
    Args:
        scraped_subreddit_posts : [[subquery,total_hits],...]
    '''
    # Create DataFrame
    df_count_posts = pd.DataFrame(scraped_subreddits_posts, columns=['subreddit', 'count'])
    
    # Clean subreddit names
    df_count_posts['subreddit'] = df_count_posts['subreddit'].str.replace('subreddit:', '', regex=False)
    
    # Sort by count in descending order
    df_count_posts = df_count_posts.sort_values('count', ascending=False)
    
    # Visualize
    plt.figure(figsize=(12, 6))
    plt.bar(df_count_posts['subreddit'], df_count_posts['count'], color='skyblue')
    
    plt.xlabel('Subreddit')
    plt.ylabel('Anzahl der Einträge')
    plt.title('Anzahl der Einträge pro Subreddit')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

```
```{python}
# Retrieve data from Elasticsearch
def get_data(index,q):
    # Retrieve data from the index
    response = search_data(index=index, q=q)
    hits = response['hits']['hits']
    return hits

def get_timeline_data(index,q):
    hits = get_data(index,q)
    # Extract relevant fields
    data = []
    for hit in hits:
        source = hit['_source']
        data.append({
            'subreddit': source.get('subreddit'),
            'date': source.get('date').split("T")[0]  # Extract the date part
        })
    
    return data

# Process data
def process_data(data):
    df = pd.DataFrame(data)
    df['date'] = pd.to_datetime(df['date'])  # Ensure the date column is in datetime format
    
    # Group by subreddit and date, then count posts
    timeline = df.groupby(['subreddit', 'date']).size().reset_index(name='post_count')
    return timeline

# Plot data
def plot_timeline(timeline):
    subreddits = timeline['subreddit'].unique()
    
    # Define the size of the figure
    num_subreddits = len(subreddits)
    fig, axes = plt.subplots(nrows=num_subreddits, ncols=1, figsize=(10, 4 * num_subreddits))
    
    # If only one subplot, axes is not a list, so wrap it in a list
    if num_subreddits == 1:
        axes = [axes]
    
    for ax, subreddit in zip(axes, subreddits):
        subreddit_data = timeline[timeline['subreddit'] == subreddit]
        ax.plot(subreddit_data['date'], subreddit_data['post_count']) 
        ax.set_title(f"Posts Per Day for {subreddit}", fontsize=14)
        ax.set_xlabel("Date", fontsize=10)
        ax.set_ylabel("Number of Posts", fontsize=10)
        ax.grid(True)
    
    # Adjust layout for better spacing
    plt.tight_layout()
    plt.show()
```
```{python}
#visualize count per subreddit
data_counts = visualize_subreddit_counts(scraped_subreddits_posts)
#visualize timeline count for the first 3 subreddit
data_timeline = get_timeline_data('wdb','*')
timeline = process_data(data_timeline)
plot_timeline(timeline)
```
## Sentiment Analysis
```{python}
def get_data_for_sentiment(index,q):
    hits_ = get_data(index,q)

    hits_
    data_sentiment = []
    for hit in hits_:
        source = hit['_source']
        data_sentiment.append({
            'id': source.get('id'),
            'title': source.get('title')
        })
    return data_sentiment

```
```{python}
data_sentiment = get_data_for_sentiment('wdb','*')
```
```{python}
sentiment_texts = [d["title"] for d in data_sentiment]

scores = sentiment.sentiment(sentiment_texts)
scores
```

