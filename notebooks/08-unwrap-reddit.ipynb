{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from CryptoFraudDetection.utils.enums import LoggerMode\n",
        "from CryptoFraudDetection.utils.logger import Logger\n",
        "from CryptoFraudDetection.elasticsearch.data_retrieval import search_data\n",
        "from CryptoFraudDetection.elasticsearch.data_insertion import insert_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = search_data(index=\"reddit_posts\", q=\"*\", size=6000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response[\"hits\"][\"hits\"][0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response[\"hits\"][\"hits\"][0][\"_source\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recursive function to flatten comments with parent_id\n",
        "def flatten_comments(comments, parent_id):\n",
        "    rows = []\n",
        "    for comment in comments:\n",
        "        # Extract comment details\n",
        "        rows.append(\n",
        "            {\n",
        "                \"id\": comment[\"id\"],\n",
        "                \"parent_id\": parent_id,\n",
        "                \"author\": comment[\"author\"],\n",
        "                \"body\": comment[\"body\"],\n",
        "                \"created\": comment[\"created\"],\n",
        "                \"depth\": comment[\"depth\"],\n",
        "                \"edited\": comment[\"edited\"],\n",
        "                \"score\": comment[\"score\"],\n",
        "                \"search_query\": comment[\"search_query\"],\n",
        "                \"subreddit\": comment[\"subreddit\"],\n",
        "            }\n",
        "        )\n",
        "        # If the comment has nested replies, process them recursively\n",
        "        if comment.get(\"comments\"):\n",
        "            rows.extend(flatten_comments(comment[\"comments\"], parent_id=comment[\"id\"]))\n",
        "    return rows\n",
        "\n",
        "\n",
        "# Function to flatten the entire JSON structure\n",
        "def flatten_json(json_data):\n",
        "    # Extract submission data\n",
        "    submission = {\n",
        "        \"id\": json_data[\"id\"],\n",
        "        \"parent_id\": None,\n",
        "        \"author\": json_data[\"author\"],\n",
        "        \"body\": json_data[\"body\"],\n",
        "        \"created\": json_data[\"created\"],\n",
        "        \"depth\": json_data[\"depth\"],\n",
        "        \"edited\": json_data[\"edited\"],\n",
        "        \"score\": json_data[\"score\"],\n",
        "        \"search_query\": json_data[\"search_query\"],\n",
        "        \"subreddit\": json_data[\"subreddit\"],\n",
        "        \"title\": json_data[\"title\"],  # Specific to submission\n",
        "        \"url\": json_data[\"url\"],  # Specific to submission\n",
        "        \"num_comments\": json_data[\"num_comments\"],  # Specific to submission\n",
        "    }\n",
        "\n",
        "    # Flatten comments\n",
        "    comments = flatten_comments(json_data[\"comments\"], parent_id=json_data[\"id\"])\n",
        "\n",
        "    # Combine submission and comments into a single dataset\n",
        "    all_data = [submission] + comments\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    return pd.DataFrame(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "for post in tqdm(response[\"hits\"][\"hits\"]):\n",
        "    if df.empty:\n",
        "        df = flatten_json(post[\"_source\"])\n",
        "    else:\n",
        "        df = pd.concat([df, flatten_json(post[\"_source\"])])\n",
        "\n",
        "df = df.convert_dtypes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_parquet(\"../data/processed/reddit_posts.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger_ = Logger(name=\"reddit_unwrap_posts\", level=LoggerMode.DEBUG, log_dir=\"../logs\")\n",
        "\n",
        "_ = insert_dataframe(logger=logger_, index=\"reddit_posts_unwrapped\", df=df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "/Users/gabriel.torres/dev/com.github/CryptoFraudDetection/main/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
