{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "import praw\n",
        "import praw.models\n",
        "import praw.models.comment_forest\n",
        "import pandas as pd\n",
        "from prawcore.exceptions import TooManyRequests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "from CryptoFraudDetection.utils.logger import Logger\n",
        "from CryptoFraudDetection.utils.enums import LoggerMode\n",
        "from CryptoFraudDetection.elasticsearch.data_retrieval import search_data\n",
        "from CryptoFraudDetection.elasticsearch.data_insertion import insert_dataframe\n",
        "\n",
        "logger_ = Logger(\n",
        "    name=\"scrape_reddit_posts\", level=LoggerMode.DEBUG, log_dir=\"../logs\"\n",
        ")\n",
        "\n",
        "# Load environment variables\n",
        "dotenv_path = find_dotenv()\n",
        "if dotenv_path and os.getenv(\"REDDIT_CLIENT_ID\") is None:\n",
        "    load_dotenv(dotenv_path)\n",
        "\n",
        "# Initialize the Reddit API client\n",
        "reddit = praw.Reddit(\n",
        "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
        "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
        "    user_agent=os.getenv(\"REDDIT_USER_AGENT\"),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def convert_unix_date(date: str) -> str:\n",
        "    if date is False:\n",
        "        return None\n",
        "    return datetime.datetime.fromtimestamp(date, datetime.timezone.utc).strftime(\n",
        "        \"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "\n",
        "def serialize_comment(comments: praw.models.comment_forest.CommentForest, search_query: str) -> dict:\n",
        "    serialized_comments = []\n",
        "    for comment in comments:\n",
        "        serialized_replies = serialize_comment(comment.replies, search_query)\n",
        "        serialized_comments.append(\n",
        "            {\n",
        "                \"author\": str(comment.author),\n",
        "                \"body\": comment.body,\n",
        "                \"comments\": serialized_replies if serialized_replies else None,\n",
        "                \"created\": convert_unix_date(comment.created_utc),\n",
        "                \"edited\": convert_unix_date(comment.edited),\n",
        "                \"depth\": comment.depth,\n",
        "                \"downs\": comment.downs,\n",
        "                \"id\": comment.id,\n",
        "                \"score\": comment.score,\n",
        "                \"search_query\": search_query,\n",
        "                \"subreddit\": comment.subreddit_name_prefixed,\n",
        "                \"ups\": comment.ups,\n",
        "            }\n",
        "        )\n",
        "    return serialized_comments\n",
        "\n",
        "\n",
        "def serialize_submission(post: praw.models.Submission, search_query: str) -> dict:\n",
        "    return {\n",
        "        \"author\": str(post.author),\n",
        "        \"body\": post.selftext,\n",
        "        \"comments\": serialize_comment(post.comments, search_query),\n",
        "        \"created\": convert_unix_date(post.created_utc),\n",
        "        \"edited\": convert_unix_date(post.edited),\n",
        "        \"depth\": -1,\n",
        "        \"downs\": post.downs,\n",
        "        \"id\": post.id,\n",
        "        \"num_comments\": post.num_comments,  # not available in comments\n",
        "        \"score\": post.score,\n",
        "        \"search_query\": search_query,\n",
        "        \"subreddit\": post.subreddit_name_prefixed,\n",
        "        \"title\": post.title,  # not available in comments\n",
        "        \"ups\": post.ups,\n",
        "        \"url\": post.url,  # not available in comments\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_results = 10_000\n",
        "response = search_data(\"reddit_metadata_100\", \"*\", max_results).body[\"hits\"][\"hits\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in response:\n",
        "    scrape_successful = False\n",
        "    while not scrape_successful:\n",
        "        try:\n",
        "            extracted_source = result[\"_source\"]\n",
        "            post_id = extracted_source[\"link\"].split(\"/\")[6]\n",
        "            search_query = extracted_source[\"query\"].split(\"site:reddit.com/r/\")[0].strip()\n",
        "            if len(post_id) not in (6,7):\n",
        "                scrape_successful = True\n",
        "                continue\n",
        "\n",
        "            post = reddit.submission(id=post_id)\n",
        "            post.comments.replace_more(limit=5)\n",
        "            serialized_post = serialize_submission(post, search_query)\n",
        "            serialized_post_df = pd.DataFrame.from_dict(serialized_post, orient='index').T\n",
        "\n",
        "            insert_dataframe(logger=logger_, index=\"reddit_posts_100\", df=serialized_post_df)\n",
        "            logger_.info(f\"Inserted post {post_id} into Elasticsearch\")\n",
        "            time.sleep(1)\n",
        "            scrape_successful = True\n",
        "\n",
        "        except IndexError:\n",
        "            scrape_successful = True\n",
        "            continue # Not a post\n",
        "\n",
        "        except TimeoutError:\n",
        "            logger_.error(\"Probably elasticsearch is down, sleeping for 2 minutes\")\n",
        "            time.sleep(120)\n",
        "\n",
        "        except TooManyRequests:\n",
        "            logger_.error(\"Too many requests, sleeping for 2 minutes\")\n",
        "            time.sleep(120)\n",
        "            continue\n",
        "\n",
        "        except Exception as e:\n",
        "            logger_.error(f\"Another error: {e}\")\n",
        "            time.sleep(1)\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/gabriel.torres/dev/com.github/CryptoFraudDetection/main/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}