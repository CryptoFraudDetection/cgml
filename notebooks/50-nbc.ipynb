{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import swifter  # noqa: F401\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MIN_COMMENTS_PER_SUBSET = 50\n",
        "N_OF_SPLITS_PER_SUBSET = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_reddit = pd.read_parquet(\"../data/processed/reddit_posts.parquet\")\n",
        "df_reddit[\"created\"] = pd.to_datetime(df_reddit[\"created\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "df_reddit[\"edited\"] = pd.to_datetime(df_reddit[\"edited\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "df_reddit.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_coins = pd.read_json(\"../data/raw/coins.json\")\n",
        "df_coins[\"start_date\"] = pd.to_datetime(df_coins[\"start_date\"], format=\"ISO8601\")\n",
        "df_coins[\"end_date\"] = pd.to_datetime(df_coins[\"end_date\"], format=\"ISO8601\")\n",
        "df_coins = df_coins.set_index(\"name\")\n",
        "\n",
        "df_coins.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_number_of_comments_per_coin(name: str, cutoff_date: str) -> int:\n",
        "    \"\"\"\n",
        "    Get the number of comments for a given coin.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the coin.\n",
        "        cutoff_date (str): The cutoff date for the comments.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of comments for the given coin.\n",
        "\n",
        "    \"\"\"\n",
        "    cutoff_date = pd.to_datetime(cutoff_date, format=\"%Y-%m-%d\")\n",
        "    return len(df_reddit[(df_reddit[\"search_query\"] == name) & (df_reddit[\"created\"] <= cutoff_date)])\n",
        "\n",
        "\n",
        "def get_first_comment_date(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the first comment date for a given coin.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the coin.\n",
        "\n",
        "    Returns:\n",
        "        str: The first comment date for the given coin.\n",
        "\n",
        "    \"\"\"\n",
        "    return df_reddit[df_reddit[\"search_query\"] == name][\"created\"].min()\n",
        "\n",
        "\n",
        "def get_last_comment_date(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the last comment date for a given coin.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the coin.\n",
        "\n",
        "    Returns:\n",
        "        str: The last comment date for the given coin.\n",
        "\n",
        "    \"\"\"\n",
        "    mod_df_reddit = df_reddit.copy()\n",
        "    mod_df_reddit[\"edited\"] = mod_df_reddit[\"edited\"].fillna(mod_df_reddit[\"created\"])\n",
        "    return mod_df_reddit[mod_df_reddit[\"search_query\"] == name][\"edited\"].max()\n",
        "\n",
        "\n",
        "def get_coin_info(name: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Get coin information for a given coin name.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the coin.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The coin information.\n",
        "\n",
        "    \"\"\"\n",
        "    return df_coins.loc[name]\n",
        "\n",
        "\n",
        "def get_coin_info_row(row: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Get coin information for a given row.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row from the dataframe containing a search_query column.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The coin information corresponding to the search_query.\n",
        "\n",
        "    \"\"\"\n",
        "    return df_coins.loc[row.search_query]\n",
        "\n",
        "\n",
        "def is_comment_valid_row(row: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Check if a comment is valid based on the coin's start and end dates.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row from the dataframe containing created and edited columns.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the comment is valid, False otherwise.\n",
        "\n",
        "    \"\"\"\n",
        "    coin = get_coin_info_row(row)\n",
        "\n",
        "    comment_date = row.edited if pd.notna(row.edited) else row.created\n",
        "    start_date_valid = comment_date >= coin[\"start_date\"]\n",
        "    end_date_valid = comment_date <= coin[\"end_date\"] if pd.notna(coin[\"end_date\"]) else True\n",
        "\n",
        "    return start_date_valid and end_date_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_reddit[\"test\"] = df_reddit.swifter.apply(get_coin_info_row, axis=1)[\"test\"]\n",
        "df_reddit[\"valid\"] = df_reddit.swifter.apply(is_comment_valid_row, axis=1)\n",
        "\n",
        "df_reddit = df_reddit[df_reddit[\"valid\"]]\n",
        "df_reddit = df_reddit.drop(columns=[\"valid\"])\n",
        "df_reddit = df_reddit.reset_index(drop=True)\n",
        "\n",
        "df_reddit.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_reddit[\"search_query\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df = df_reddit[~df_reddit[\"test\"]]\n",
        "test_df = df_reddit[df_reddit[\"test\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_train_coins = train_df[\"search_query\"].unique().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Create subsets\n",
        "list_subsets = []\n",
        "for coin in unique_train_coins:\n",
        "    # Get metadata for the coin\n",
        "    metadata_coin = get_coin_info(coin).copy()\n",
        "    metadata_coin[\"start_date\"] = get_first_comment_date(coin)\n",
        "    metadata_coin[\"end_date\"] = get_last_comment_date(coin)\n",
        "\n",
        "    # Create N_OF_SPLITS_PER_SUBSET splits\n",
        "    for _ in range(N_OF_SPLITS_PER_SUBSET):\n",
        "        successful_split = False\n",
        "        while not successful_split:\n",
        "            # Get a random date between the start and end date\n",
        "            random_day_diff = random.randint(0, (metadata_coin[\"end_date\"] - metadata_coin[\"start_date\"]).days)\n",
        "            random_date = metadata_coin[\"start_date\"].date() + pd.Timedelta(days=random_day_diff)\n",
        "\n",
        "            # Check if there are enough comments for the subset to be valid\n",
        "            if get_number_of_comments_per_coin(coin, random_date) > MIN_COMMENTS_PER_SUBSET:\n",
        "                successful_split = True\n",
        "\n",
        "        # Append the subset to the list\n",
        "        list_subsets.append({\"coin\": coin, \"split_date\": random_date, \"fraud\": metadata_coin[\"fraud\"]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Drop duplicates\n",
        "df_subsets = pd.DataFrame(list_subsets)\n",
        "df_subsets = df_subsets.drop_duplicates()\n",
        "df_subsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_subsets.coin.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectorizer = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    analyzer=\"word\",\n",
        "    lowercase=True,\n",
        ")\n",
        "\n",
        "vectorizer.fit(train_df[\"body\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Number of features:\", len(vectorizer.vocabulary_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_text_from_split(data: pd.DataFrame, split_date: str, coin: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the text data for a given coin and split date.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): The dataframe containing the data.\n",
        "        split_date (str): The split date.\n",
        "        coin (str): The coin name.\n",
        "\n",
        "    Returns:\n",
        "        str: The text data for the given coin and split date\n",
        "\n",
        "    \"\"\"\n",
        "    split_date = pd.to_datetime(split_date, format=\"%Y-%m-%d\")\n",
        "\n",
        "    data = data.copy()\n",
        "    data[\"edited\"] = data[\"edited\"].fillna(data[\"created\"])\n",
        "    data = data[data[\"search_query\"] == coin]\n",
        "    data = data[data[\"edited\"] <= split_date]\n",
        "    return data[\"body\"].str.cat(sep=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# LOOCV based on each coin\n",
        "metrics = defaultdict(dict)\n",
        "\n",
        "for i, coin in tqdm(enumerate(df_subsets.coin.unique()), total=len(df_subsets.coin.unique())):\n",
        "    # Get the fitting and validation splits\n",
        "    fit_df = df_subsets[df_subsets[\"coin\"] != coin]\n",
        "    val_df = df_subsets[df_subsets[\"coin\"] == coin]\n",
        "\n",
        "    # Get the fitting data\n",
        "    list_fitting_data = []\n",
        "    for _, row in fit_df.iterrows():\n",
        "        list_fitting_data.append(\n",
        "            {\n",
        "                \"coin\": row[\"coin\"],\n",
        "                \"split_date\": row[\"split_date\"],\n",
        "                \"text\": get_text_from_split(train_df, row[\"split_date\"], row[\"coin\"]),\n",
        "                \"fraud\": row[\"fraud\"],\n",
        "            },\n",
        "        )\n",
        "\n",
        "    # Get the validation data\n",
        "    list_validation_data = []\n",
        "    for _, row in val_df.iterrows():\n",
        "        list_validation_data.append(\n",
        "            {\n",
        "                \"coin\": row[\"coin\"],\n",
        "                \"split_date\": row[\"split_date\"],\n",
        "                \"text\": get_text_from_split(train_df, row[\"split_date\"], row[\"coin\"]),\n",
        "                \"fraud\": row[\"fraud\"],\n",
        "            },\n",
        "        )\n",
        "\n",
        "    # Create the fitting and validation dataframes\n",
        "    df_fitting = pd.DataFrame(list_fitting_data)\n",
        "    df_validation = pd.DataFrame(list_validation_data)\n",
        "    del list_fitting_data\n",
        "    del list_validation_data\n",
        "\n",
        "    # fitting Multinomial Naive Bayes\n",
        "    clf = MultinomialNB()\n",
        "    X_fitting = vectorizer.transform(df_fitting[\"text\"])\n",
        "    y_fitting = df_fitting[\"fraud\"]\n",
        "    clf.fit(X_fitting, y_fitting)\n",
        "\n",
        "    # Predicting the validation set\n",
        "    X_validation = vectorizer.transform(df_validation[\"text\"])\n",
        "    y_validation = df_validation[\"fraud\"].to_numpy()\n",
        "    y_pred = clf.predict_proba(X_validation)\n",
        "\n",
        "    # calculating metrics for left out coin\n",
        "    metrics[i] = {\n",
        "        \"coin_left_out\": coin,\n",
        "        \"coin_split_date\": df_validation[\"split_date\"].to_list(),\n",
        "        \"coin_fraud\": df_validation[\"fraud\"].to_list(),\n",
        "        \"accuracy\": 1 * (y_pred[:, 1].round() == y_validation),\n",
        "        \"y_pred\": y_pred[:, 1].round(3),\n",
        "        \"y_true\": y_validation,\n",
        "    }\n",
        "\n",
        "    # deleting dataframes to free up memory\n",
        "    del df_fitting\n",
        "    del df_validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# idea: for each coin, visualize until which date the model is able to predict frauds\n",
        "import altair as alt\n",
        "\n",
        "for i in metrics.keys():\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics[i])\n",
        "    df_metrics[\"coin_fraud\"] = df_metrics[\"coin_fraud\"].astype(int)\n",
        "    df_metrics[\"coin_split_date\"] = pd.to_datetime(df_metrics[\"coin_split_date\"])\n",
        "\n",
        "    chart = (\n",
        "        alt.Chart(df_metrics)\n",
        "        .mark_area(\n",
        "            interpolate=\"step-after\",\n",
        "            opacity=0.6,\n",
        "            line=True,\n",
        "        )\n",
        "        .encode(\n",
        "            x=alt.X(\"coin_split_date\", title=\"Date\").axis(format=\"%Y-%m-%d\", labelAngle=-90),\n",
        "            y=alt.Y(\"accuracy\", title=\"Accuracy\", scale=alt.Scale(domain=[0,1])).axis(format=\"%\"),\n",
        "            tooltip=[\n",
        "                alt.Tooltip(\"coin_split_date\", title=\"Date\", format=\"%Y-%m-%d\"),\n",
        "                alt.Tooltip(\"accuracy\", title=\"Accuracy\"),\n",
        "            ],\n",
        "        )\n",
        "        .properties(title=f\"Accuracy for {df_metrics['coin_left_out'].iloc[0]}, correct label: {\"fraud\" if df_metrics['coin_fraud'].iloc[0] else \"not fraud\"}\",\n",
        "                    width=500,\n",
        "                    height=50)\n",
        "    )\n",
        "\n",
        "    display(chart)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/gabrieltorresgamez/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}