{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "from CryptoFraudDetection.utils.logger import Logger\n",
        "from CryptoFraudDetection.utils.exceptions import ProxyNotWorking, DetectedBotException\n",
        "from CryptoFraudDetection.utils.enums import ScraperNotebookMode, LoggerMode\n",
        "from CryptoFraudDetection.elasticsearch.data_insertion import insert_dict\n",
        "from CryptoFraudDetection.scraper.google_results import GoogleResultsScraper\n",
        "from CryptoFraudDetection.scraper.utils import get_driver\n",
        "\n",
        "import pandas as pd\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "logger_ = Logger(name=\"scrape_reddit_metadata\", level=LoggerMode.DEBUG, log_dir=\"../logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "MODE = ScraperNotebookMode.WRITE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read data/raw/coins.json\n",
        "with open('../data/raw/coins.json') as f:\n",
        "    coins = json.load(f)\n",
        "coins = sorted(coins, key=lambda coin: coin['max_market_cap_e9'], reverse=False)\n",
        "coins[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_next_proxy(\n",
        "    link=\"https://api.proxyscrape.com/v4/free-proxy-list/get?request=display_proxies&proxy_format=protocolipport&format=csv&timeout=2000\",\n",
        "):\n",
        "    proxy_list = pd.read_csv(link)\n",
        "    proxy_list = proxy_list.sample(1)\n",
        "    return proxy_list.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "proxy_info = get_next_proxy()\n",
        "logger_.info(f\"Using proxy {proxy_info.protocol}://{proxy_info.ip}:{proxy_info.port}\")\n",
        "\n",
        "N_SITES = 100\n",
        "if MODE == ScraperNotebookMode.WRITE:\n",
        "    for coin in coins:\n",
        "        for subreddit in coin['subreddits']:\n",
        "            scrape_successful = False\n",
        "            while not scrape_successful:\n",
        "                try:\n",
        "                    logger_.info(f\"Scraping {coin['name']} in {subreddit}\")\n",
        "                    query = f\"{coin['name']} site:reddit.com/r/{subreddit} {\"before:\" + coin['end_date'] if coin.get('end_date') else \"\"} {\"after:\" + coin['start_date'] }\"\n",
        "                    query = query.replace(\"  \", \" \")\n",
        "                    logger_.debug(f\"Query: {query}\")\n",
        "                    scraper = GoogleResultsScraper(logger=logger_)\n",
        "                    results = scraper.get_main_results(\n",
        "                        query,\n",
        "                        n_sites=N_SITES,\n",
        "                        headless=True,\n",
        "                        proxy_protocol=proxy_info.protocol,\n",
        "                        proxy_address=f\"{proxy_info.ip}:{proxy_info.port}\",\n",
        "                    )\n",
        "                    insert_dict(\n",
        "                        logger=logger_, index=\"reddit_metadata_100\", data_dict=results\n",
        "                    )\n",
        "                    scrape_successful = True\n",
        "                    time.sleep(5)\n",
        "                except Exception as e:\n",
        "                    logger_.warning(\"Detected bot, proxy not working or other error\")\n",
        "                    proxy_info = get_next_proxy()\n",
        "                    logger_.info(\n",
        "                       f\"Using proxy {proxy_info.protocol}://{proxy_info.ip}:{proxy_info.port}\"\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/gabriel.torres/dev/com.github/CryptoFraudDetection/main/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}