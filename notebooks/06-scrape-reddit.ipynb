{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from CryptoFraudDetection.elasticsearch.data_insertion import insert_dict\n",
        "from CryptoFraudDetection.scraper.google_results import GoogleResultsScraper\n",
        "from CryptoFraudDetection.utils.enums import LoggerMode, ScraperNotebookMode\n",
        "from CryptoFraudDetection.utils.logger import Logger\n",
        "\n",
        "logger_ = Logger(name=\"scrape_reddit_metadata\", level=LoggerMode.DEBUG, log_dir=\"../logs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODE = ScraperNotebookMode.WRITE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'Safe Moon',\n",
              " 'symbol': 'SAFEMOON',\n",
              " 'fraud': True,\n",
              " 'test': True,\n",
              " 'max_market_cap_e9': 0.0,\n",
              " 'start_date': '2022-01-17',\n",
              " 'subreddits': ['CryptoCurrency',\n",
              "  'CryptoMoonShots',\n",
              "  'CryptoMarkets',\n",
              "  'Crypto',\n",
              "  'SafeMoon'],\n",
              " 'end_date': '2023-10-31'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read data/raw/coins.json\n",
        "with open(\"../data/raw/coins.json\") as f:\n",
        "    coins = json.load(f)\n",
        "coins = sorted(coins, key=lambda coin: coin[\"max_market_cap_e9\"], reverse=False)\n",
        "coins[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_next_proxy(\n",
        "    link=\"https://api.proxyscrape.com/v4/free-proxy-list/get?request=display_proxies&proxy_format=protocolipport&format=csv&timeout=2000\",\n",
        "):\n",
        "    proxy_list = pd.read_csv(link)\n",
        "    proxy_list = proxy_list.sample(1)\n",
        "    return proxy_list.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proxy_info = get_next_proxy()\n",
        "logger_.info(f\"Using proxy {proxy_info.protocol}://{proxy_info.ip}:{proxy_info.port}\")\n",
        "\n",
        "N_SITES = 100\n",
        "if MODE == ScraperNotebookMode.WRITE:\n",
        "    for coin in coins:\n",
        "        for subreddit in coin[\"subreddits\"]:\n",
        "            scrape_successful = False\n",
        "            while not scrape_successful:\n",
        "                try:\n",
        "                    logger_.info(f\"Scraping {coin['name']} in {subreddit}\")\n",
        "                    query = f\"{coin['name']} site:reddit.com/r/{subreddit} {\"before:\" + coin['end_date'] if coin.get('end_date') else \"\"} {\"after:\" + coin['start_date'] }\"\n",
        "                    query = query.replace(\"  \", \" \")\n",
        "                    logger_.debug(f\"Query: {query}\")\n",
        "                    scraper = GoogleResultsScraper(logger=logger_)\n",
        "                    results = scraper.get_main_results(\n",
        "                        query,\n",
        "                        n_sites=N_SITES,\n",
        "                        headless=True,\n",
        "                        proxy_protocol=proxy_info.protocol,\n",
        "                        proxy_address=f\"{proxy_info.ip}:{proxy_info.port}\",\n",
        "                    )\n",
        "                    insert_dict(\n",
        "                        logger=logger_,\n",
        "                        index=\"reddit_metadata_100\",\n",
        "                        data_dict=results,\n",
        "                    )\n",
        "                    scrape_successful = True\n",
        "                    time.sleep(5)\n",
        "                except Exception as e:\n",
        "                    logger_.warning(\"Detected bot, proxy not working or other error\")\n",
        "                    proxy_info = get_next_proxy()\n",
        "                    logger_.info(f\"Using proxy {proxy_info.protocol}://{proxy_info.ip}:{proxy_info.port}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
