---
title: Scrape Reddit Posts
jupyter: python3
---

```{python}
import json

from CryptoFraudDetection.utils.enums import ScraperNotebookMode, LoggerMode
from CryptoFraudDetection.utils.logger import Logger
from CryptoFraudDetection.scraper.reddit import RedditScraper
from CryptoFraudDetection.elasticsearch.data_insertion import insert_dataframe

logger = Logger(name="scrape_reddit", level=LoggerMode.WARNING, log_dir="../logs")
```

```{python}
# Enable or disable scraping
MODE = ScraperNotebookMode.WRITE
# Number of subreddits and queries to scrape
# set to None to scrape all posts
LIMIT = None
```

```{python}
subreddits: list[str] = [
    "r/CryptoCurrency",
    "r/CryptoMoonShots",
    "r/CryptoMarkets",
    "r/Crypto",
    "r/Ethereum",
    "r/Bitcoin",
    "r/btc",
    "r/litecoin",
    "r/ethtrader",
    "r/Ripple",
    "r/BitcoinMarkets",
    "r/altcoin",
    "r/binance",
]

coins: list[str] = [
    "Bitcoin",
    "BTC",
    "Ethereum",
    "ETH",
    "Cosmos",
    "ATOM",
    "Avalanche",
    "AVAX",
    "FTX Token",
    "FTT",
    "Terra Luna Classic",
    "Terra Luna",
    "Terra Classic",
    "LUNC",
    "Squid-Game-Coin",
    "SQUID",
    "BeerCoin",
    "BEER",
    "BitForex",
    "BF",
    "BeerCoin",
    "BEER",
    "Safe Moon",
    "SAFEMOON",
    "Teddy Doge",
    "TEDDY V2",
    "STOA Network",
    "STA",
    "Chainlink",
    "LINK",
    "Polkadot",
    "DOT",
    "Solana",
    "SOL",
    "THORChain",
    "RUNE",
    "Avalanche",
    "AVAX",
    "Algorand",
    "ALGO",
    "Polygon",
    "MATIC",
    "Cardano",
    "ADA",
    "VeChain",
    "VET",
]
```

Testing until reddit blocks:

```{python}
subreddits_ = subreddits[:LIMIT] if LIMIT else subreddits
coins_ = coins[:LIMIT] if LIMIT else coins
```

```{python}
posts = {}
if MODE == ScraperNotebookMode.WRITE:
    for sub in subreddits_:
        for coin in coins_:
            # TODO: write as df instead of json?
            # TODO: write about failure strat and saving to json
            sub_ = sub.split("/")[-1].lower()
            coin_ = coin.lower()
            json_path = f"../data/reddit/{sub_}_{coin_}.json"
            try:  # if file already exists, skip scraping
                data = json.load(open(json_path))
                # TODO: logger?
                print(
                    f"Skipping scraping for sub: {sub}, coin: {coin} as is could be read from file."
                )
            except:  # scrape and save to file
                scraper = RedditScraper(logger, headless=True)
                scraper.scrape(sub, coin)
                data = scraper.post_data
                with open(json_path, "w") as f:
                    f.write(json.dumps(data, indent=4))

            posts[(sub, coin)] = data
            print(f"sub: {sub}, coin: {coin}, len data: {len(data)}")
```

```{python}
posts
```

