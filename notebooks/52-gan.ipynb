{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from CryptoFraudDetection.utils.enums import LoggerMode\n",
    "from CryptoFraudDetection.utils.logger import Logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "logger_ = Logger(name=\"graph_attention_network\", level=LoggerMode.WARNING, log_dir=\"../logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_exists = None\n",
    "df_reddit = None\n",
    "\n",
    "try:\n",
    "    df_reddit = pd.read_parquet(\n",
    "        \"../data/processed/reddit_posts_embedded.parquet\",\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"parent_id\",\n",
    "            \"author\",\n",
    "            \"score\",\n",
    "            \"search_query\",\n",
    "            \"subreddit\",\n",
    "            \"test\",\n",
    "            \"fraud\",\n",
    "            \"embedding\",\n",
    "        ],\n",
    "    )\n",
    "    embedding_exists = True\n",
    "except FileNotFoundError:\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>search_query</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>test</th>\n",
       "      <th>fraud</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yxu5tv</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>magus-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>Safe Moon</td>\n",
       "      <td>r/CryptoCurrency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.4334820508956909, -0.4628458321094513, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id parent_id    author  score search_query         subreddit  test  \\\n",
       "0  yxu5tv      <NA>  magus-21   1597    Safe Moon  r/CryptoCurrency  True   \n",
       "\n",
       "   fraud                                          embedding  \n",
       "0   True  [-0.4334820508956909, -0.4628458321094513, -0....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    502081.0\n",
       "mean     0.052482\n",
       "std      0.003728\n",
       "min           0.0\n",
       "25%      0.052263\n",
       "50%      0.052285\n",
       "75%      0.052328\n",
       "max           1.0\n",
       "Name: normalized_score, dtype: Float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_min = df_reddit[\"score\"].min()\n",
    "score_max = df_reddit[\"score\"].max()\n",
    "df_reddit[\"normalized_score\"] = (df_reddit[\"score\"] - score_min) / (score_max - score_min)\n",
    "df_reddit[\"normalized_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>search_query</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>test</th>\n",
       "      <th>fraud</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yxu5tv</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>magus-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>Safe Moon</td>\n",
       "      <td>r/CryptoCurrency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.08661773252685279, -0.4334820508956909, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id parent_id    author  score search_query         subreddit  test  \\\n",
       "0  yxu5tv      <NA>  magus-21   1597    Safe Moon  r/CryptoCurrency  True   \n",
       "\n",
       "   fraud                                           features  \n",
       "0   True  [0.08661773252685279, -0.4334820508956909, -0....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add features column\n",
    "df_reddit[\"features\"] = df_reddit.apply(\n",
    "    lambda row: np.concatenate([np.array([row[\"normalized_score\"]]), np.array(row[\"embedding\"])], axis=0),\n",
    "    axis=1,\n",
    ")\n",
    "df_reddit = df_reddit.drop(columns=[\"embedding\", \"normalized_score\"])\n",
    "\n",
    "df_reddit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_reddit[~df_reddit[\"test\"]]\n",
    "df_test = df_reddit[df_reddit[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach has been scraped, since it creates hundreds of millions of edges and fills up my whole memory\n",
    "# The idea was to create edges between all nodes that share the same author, subreddit or search query\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def createedges_from_group(df, group_feature):\n",
    "#     \"\"\"Create edges by connecting all nodes within the same group without duplicates.\"\"\"\n",
    "#     edges = []\n",
    "#     unique_groups = sorted(df[group_feature].unique())  # Sort groups for ordered processing\n",
    "#\n",
    "#     for group in tqdm(unique_groups, desc=f\"Processing {group_feature}\"):\n",
    "#         # Filter IDs for the current group\n",
    "#         group_ids = sorted(df[df[group_feature] == group][\"id\"].to_numpy())  # Sort node IDs\n",
    "#\n",
    "#         # Process connections for each node\n",
    "#         while group_ids:  # Keep processing until all nodes in this group are handled\n",
    "#             current_node = group_ids.pop(0)  # Take the first node and \"pop\" it from the list\n",
    "#             for target_node in group_ids:  # Connect it with all remaining nodes\n",
    "#                 edges.append((current_node, target_node))\n",
    "#     return edges\n",
    "#\n",
    "#\n",
    "# # Same-author, same-subreddit, same-search_query edges (bidirectional)\n",
    "# edges_same_author = createedges_from_group(df_reddit, \"author\")\n",
    "# edges_same_subreddit = createedges_from_group(df_reddit, \"subreddit\")\n",
    "# edges_same_query = createedges_from_group(df_reddit, \"search_query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # GAT layers\n",
    "        self.gat = torch_geometric.nn.models.GAT(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_layers=num_layers,\n",
    "            v2=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply GAT layers\n",
    "        x = self.gat(x, edge_index)\n",
    "\n",
    "        # Return the output with a sigmoid activation\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class RedditDataset(Data):\n",
    "    def __init__(self, data, device) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.edge_index = None\n",
    "        self.device = device\n",
    "\n",
    "    def process(self) -> None:\n",
    "        # reset index\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "\n",
    "        # create mapping\n",
    "        id_mapping = dict(zip(self.data[\"id\"].values, self.data.index.values))\n",
    "\n",
    "        # replace hashes with indices\n",
    "        self.data[\"id\"] = self.data[\"id\"].map(id_mapping).astype(int)\n",
    "        self.data[\"parent_id\"] = self.data[\"parent_id\"].map(id_mapping).fillna(-1).astype(int)\n",
    "\n",
    "        # create edges\n",
    "        edges = pd.concat([self.data[\"id\"], self.data[\"parent_id\"]], axis=1)\n",
    "        edges = edges[edges[\"parent_id\"] != -1]\n",
    "        edges = edges.to_numpy()\n",
    "\n",
    "        # prepare data\n",
    "        self.x = torch.tensor(np.array(self.data[\"features\"].to_list()), dtype=torch.float).to(self.device)\n",
    "        self.y = torch.tensor(self.data[\"fraud\"].to_list(), dtype=torch.float).to(self.device)\n",
    "        self.edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(self.device)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(df, coin, device, balance_labels, logger_):\n",
    "    \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "    logger_.debug(f\"Preparing datasets for coin: {coin}\")\n",
    "    fit_data = df[df[\"search_query\"] != coin]\n",
    "\n",
    "    if balance_labels:\n",
    "        fit_data = pd.concat(\n",
    "            [\n",
    "                fit_data[fit_data[\"fraud\"] == 1],\n",
    "                fit_data[fit_data[\"fraud\"] == 0].sample(n=fit_data[fit_data[\"fraud\"] == 1].shape[0], random_state=42),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    val_data = df[df[\"search_query\"] == coin]\n",
    "\n",
    "    fit_dataset = RedditDataset(fit_data, device)\n",
    "    fit_dataset.process()\n",
    "\n",
    "    val_dataset = RedditDataset(val_data, device)\n",
    "    val_dataset.process()\n",
    "\n",
    "    return fit_dataset, val_dataset\n",
    "\n",
    "\n",
    "def prepare_model(device, in_channels, hidden_channels, out_channels, num_layers, lr, logger_):\n",
    "    \"\"\"Initialize the GAT model, loss function, and optimizer.\"\"\"\n",
    "    logger_.debug(\n",
    "        \"Initializing model with parameters: \"\n",
    "        f\"in_channels={in_channels}, hidden_channels={hidden_channels}, \"\n",
    "        f\"out_channels={out_channels}, num_layers={num_layers}, lr={lr}\"\n",
    "    )\n",
    "    model = GAT(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=hidden_channels,\n",
    "        out_channels=out_channels,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "\n",
    "def train_model(model, dataset, criterion, optimizer, logger_):\n",
    "    \"\"\"Train the model for one iteration.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset.x, dataset.edge_index).squeeze()\n",
    "    loss = criterion(out, dataset.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    logger_.debug(f\"Training loss: {loss.item()}\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def validate_model(model, dataset, criterion, logger_):\n",
    "    \"\"\"Evaluate the model on the validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(dataset.x, dataset.edge_index).squeeze()\n",
    "        val_loss = criterion(out, dataset.y)\n",
    "        all_preds.append(out)\n",
    "        all_targets.append(dataset.y)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "    accuracy = ((all_preds >= 0.5) * 1 == all_targets).mean()\n",
    "\n",
    "    logger_.debug(f\"Validation loss: {val_loss.item()}, Validation accuracy: {accuracy}\")\n",
    "    return val_loss, accuracy\n",
    "\n",
    "\n",
    "def train_for_coin(\n",
    "    coin,\n",
    "    df,\n",
    "    device,\n",
    "    training_loops,\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    num_layers,\n",
    "    lr,\n",
    "    balance_labels,\n",
    "    logger_,\n",
    "):\n",
    "    \"\"\"Train and validate the model, leaving one coin out.\"\"\"\n",
    "    logger_.debug(f\"Training model - leaving out {coin}\")\n",
    "\n",
    "    # Prepare data\n",
    "    fit_dataset, val_dataset = prepare_datasets(df, coin, device, balance_labels, logger_)\n",
    "\n",
    "    # Prepare model\n",
    "    model, criterion, optimizer = prepare_model(\n",
    "        device,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        lr,\n",
    "        logger_,\n",
    "    )\n",
    "\n",
    "    for i in range(training_loops):\n",
    "        logger_.debug(f\"Training loop {i + 1}/{training_loops}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_model(model, fit_dataset, criterion, optimizer, logger_)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, accuracy = validate_model(model, val_dataset, criterion, logger_)\n",
    "        logger_.debug(\n",
    "            f\"End of loop {i + 1}: Training loss: {train_loss.item()}, Validation loss: {val_loss.item()}, Validation accuracy: {accuracy}\"\n",
    "        )\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main(\n",
    "    df,\n",
    "    device,\n",
    "    training_loops,\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    num_layers,\n",
    "    lr,\n",
    "    balance_labels,\n",
    "    logger_,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a GAT model for each coin in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): Input dataset.\n",
    "        device (torch.device): Device for training.\n",
    "        training_loops (int): Number of training loops.\n",
    "        in_channels (int): Input channels for the GAT model.\n",
    "        hidden_channels (int): Number of hidden channels in the GAT model.\n",
    "        out_channels (int): Number of output channels for the GAT model.\n",
    "        num_layers (int): Number of layers in the GAT model.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        balance_labels (bool): Whether to balance the labels in the fitting dataset.\n",
    "        logger_ (logging.Logger): Logger object for logging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: A dictionary where keys are coin names and values are validation accuracies.\n",
    "\n",
    "    \"\"\"\n",
    "    accuracies = {}\n",
    "    coins = df[\"search_query\"].unique()\n",
    "\n",
    "    for coin in coins:\n",
    "        accuracy = train_for_coin(\n",
    "            coin,\n",
    "            df,\n",
    "            device,\n",
    "            training_loops,\n",
    "            in_channels,\n",
    "            hidden_channels,\n",
    "            out_channels,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            balance_labels,\n",
    "            logger_,\n",
    "        )\n",
    "        accuracies[coin] = accuracy.item()\n",
    "\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Test if works\n",
    "_ = main(\n",
    "    df_train,\n",
    "    device,\n",
    "    training_loops=10,\n",
    "    in_channels=769,\n",
    "    hidden_channels=256,\n",
    "    out_channels=1,\n",
    "    num_layers=10,\n",
    "    lr=0.0005,\n",
    "    balance_labels=True,\n",
    "    logger_=logger_,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:59:20,929 - graph_attention_network - WARNING - Starting search 1/1000\n",
      "2024-12-31 15:00:34,579 - graph_attention_network - WARNING - Starting search 2/1000\n",
      "2024-12-31 15:04:27,090 - graph_attention_network - WARNING - Starting search 3/1000\n"
     ]
    }
   ],
   "source": [
    "def initialize_json_file(file_path):\n",
    "    \"\"\"Initialize the JSON file if it does not exist.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        with file_path.open(\"w\") as f:\n",
    "            json.dump([], f)  # Start with an empty list\n",
    "\n",
    "\n",
    "def append_to_json(file_path, data):\n",
    "    \"\"\"Append a new entry to the JSON file.\"\"\"\n",
    "    with file_path.open(\"r+\") as f:\n",
    "        existing_data = json.load(f)  # Load existing data\n",
    "        existing_data.append(data)  # Add the new entry\n",
    "        f.seek(0)  # Reset cursor to the beginning of the file\n",
    "        json.dump(existing_data, f, indent=4)  # Write updated data back to the file\n",
    "\n",
    "\n",
    "def perform_hyperparameter_search(searches, file_path, logger_, df_train, device, main):\n",
    "    \"\"\"Perform random hyperparameter searches and log results.\"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for i in range(searches):\n",
    "        try:\n",
    "            # Randomly sample hyperparameters\n",
    "            params = {\n",
    "                \"training_loops\": rng.choice([500, 1000, 2000, 4000]).item(),\n",
    "                \"hidden_channels\": rng.choice([8, 16, 32, 64, 128, 256]).item(),\n",
    "                \"num_layers\": rng.integers(1, 10).item(),\n",
    "                \"lr\": rng.choice([0.0001, 0.0005, 0.001, 0.005]).item(),\n",
    "                \"balance_labels\": rng.choice([True, False]).item(),\n",
    "            }\n",
    "            logger_.info(f\"Starting search {i + 1}/{searches}\")\n",
    "            logger_.info(f\"Hyperparameters: {params}\")\n",
    "\n",
    "            # Run the main function to get accuracies\n",
    "            accuracies = main(\n",
    "                df_train,\n",
    "                device,\n",
    "                training_loops=params[\"training_loops\"],\n",
    "                in_channels=769,\n",
    "                hidden_channels=params[\"hidden_channels\"],\n",
    "                out_channels=1,\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                lr=params[\"lr\"],\n",
    "                balance_labels=params[\"balance_labels\"],\n",
    "                logger_=logger_,\n",
    "            )\n",
    "\n",
    "            # Combine parameters and accuracies\n",
    "            result = {**params, \"accuracies\": accuracies}\n",
    "\n",
    "            # Append result to the JSON file\n",
    "            append_to_json(file_path, result)\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError as e:  # noqa: PERF203\n",
    "            logger_.error(f\"Out of memory error: {e}, skipping search {i + 1}/{searches}\")\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "file_path = Path(\"../reports/hyperparameter_search_gat.json\")\n",
    "initialize_json_file(file_path)\n",
    "\n",
    "perform_hyperparameter_search(\n",
    "    searches=1000,\n",
    "    file_path=file_path,\n",
    "    logger_=logger_,\n",
    "    df_train=df_train,\n",
    "    device=device,\n",
    "    main=main,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
