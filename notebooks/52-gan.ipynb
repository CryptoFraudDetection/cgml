{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532899a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:11.963037Z",
     "iopub.status.busy": "2025-01-01T19:52:11.962665Z",
     "iopub.status.idle": "2025-01-01T19:52:14.373251Z",
     "shell.execute_reply": "2025-01-01T19:52:14.372996Z"
    },
    "papermill": {
     "duration": 2.413293,
     "end_time": "2025-01-01T19:52:14.373705",
     "exception": false,
     "start_time": "2025-01-01T19:52:11.960412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "import cudf\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from CryptoFraudDetection.utils.enums import LoggerMode\n",
    "from CryptoFraudDetection.utils.logger import Logger\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "logger_ = Logger(\n",
    "    name=\"graph_attention_network\", level=LoggerMode.INFO, log_dir=\"../logs\"\n",
    ")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003fb9ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:14.377494Z",
     "iopub.status.busy": "2025-01-01T19:52:14.377334Z",
     "iopub.status.idle": "2025-01-01T19:52:24.835838Z",
     "shell.execute_reply": "2025-01-01T19:52:24.835532Z"
    },
    "papermill": {
     "duration": 10.460937,
     "end_time": "2025-01-01T19:52:24.836712",
     "exception": false,
     "start_time": "2025-01-01T19:52:14.375775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_exists = None\n",
    "df_reddit = None\n",
    "\n",
    "try:\n",
    "    df_reddit = pd.read_parquet(\n",
    "        \"../data/processed/reddit_posts_embedded.parquet\",\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"parent_id\",\n",
    "            \"author\",\n",
    "            \"score\",\n",
    "            \"search_query\",\n",
    "            \"subreddit\",\n",
    "            \"test\",\n",
    "            \"fraud\",\n",
    "            \"embedding\",\n",
    "        ],\n",
    "    )\n",
    "    embedding_exists = True\n",
    "except FileNotFoundError:\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc400a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:24.847148Z",
     "iopub.status.busy": "2025-01-01T19:52:24.846997Z",
     "iopub.status.idle": "2025-01-01T19:52:24.853704Z",
     "shell.execute_reply": "2025-01-01T19:52:24.853495Z"
    },
    "papermill": {
     "duration": 0.016047,
     "end_time": "2025-01-01T19:52:24.854198",
     "exception": false,
     "start_time": "2025-01-01T19:52:24.838151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>search_query</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>test</th>\n",
       "      <th>fraud</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yxu5tv</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>magus-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>Safe Moon</td>\n",
       "      <td>r/CryptoCurrency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.4334820508956909, -0.4628458321094513, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id parent_id    author  score search_query         subreddit  test  \\\n",
       "0  yxu5tv      <NA>  magus-21   1597    Safe Moon  r/CryptoCurrency  True   \n",
       "\n",
       "   fraud                                          embedding  \n",
       "0   True  [-0.4334820508956909, -0.4628458321094513, -0....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1eed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:24.856770Z",
     "iopub.status.busy": "2025-01-01T19:52:24.856628Z",
     "iopub.status.idle": "2025-01-01T19:52:24.868276Z",
     "shell.execute_reply": "2025-01-01T19:52:24.868073Z"
    },
    "papermill": {
     "duration": 0.013313,
     "end_time": "2025-01-01T19:52:24.868613",
     "exception": false,
     "start_time": "2025-01-01T19:52:24.855300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    502081.0\n",
       "mean     0.052482\n",
       "std      0.003728\n",
       "min           0.0\n",
       "25%      0.052263\n",
       "50%      0.052285\n",
       "75%      0.052328\n",
       "max           1.0\n",
       "Name: normalized_score, dtype: Float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_min = df_reddit[\"score\"].min()\n",
    "score_max = df_reddit[\"score\"].max()\n",
    "df_reddit[\"normalized_score\"] = (df_reddit[\"score\"] - score_min) / (\n",
    "    score_max - score_min\n",
    ")\n",
    "df_reddit[\"normalized_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3917f0b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:24.871062Z",
     "iopub.status.busy": "2025-01-01T19:52:24.870983Z",
     "iopub.status.idle": "2025-01-01T19:52:28.260923Z",
     "shell.execute_reply": "2025-01-01T19:52:28.260618Z"
    },
    "papermill": {
     "duration": 3.391679,
     "end_time": "2025-01-01T19:52:28.261377",
     "exception": false,
     "start_time": "2025-01-01T19:52:24.869698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>search_query</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>test</th>\n",
       "      <th>fraud</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yxu5tv</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>magus-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>Safe Moon</td>\n",
       "      <td>r/CryptoCurrency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.08661773252685279, -0.4334820508956909, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id parent_id    author  score search_query         subreddit  test  \\\n",
       "0  yxu5tv      <NA>  magus-21   1597    Safe Moon  r/CryptoCurrency  True   \n",
       "\n",
       "   fraud                                           features  \n",
       "0   True  [0.08661773252685279, -0.4334820508956909, -0....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add features column\n",
    "df_reddit[\"features\"] = df_reddit.apply(\n",
    "    lambda row: np.concatenate(\n",
    "        [np.array([row[\"normalized_score\"]]), np.array(row[\"embedding\"])],\n",
    "        axis=0,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "df_reddit = df_reddit.drop(columns=[\"embedding\", \"normalized_score\"])\n",
    "\n",
    "df_reddit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ca8b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:28.271806Z",
     "iopub.status.busy": "2025-01-01T19:52:28.271720Z",
     "iopub.status.idle": "2025-01-01T19:52:28.321922Z",
     "shell.execute_reply": "2025-01-01T19:52:28.321524Z"
    },
    "papermill": {
     "duration": 0.05991,
     "end_time": "2025-01-01T19:52:28.322819",
     "exception": false,
     "start_time": "2025-01-01T19:52:28.262909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_reddit[~df_reddit[\"test\"]]\n",
    "df_test = df_reddit[df_reddit[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45362a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:28.325746Z",
     "iopub.status.busy": "2025-01-01T19:52:28.325664Z",
     "iopub.status.idle": "2025-01-01T19:52:28.327345Z",
     "shell.execute_reply": "2025-01-01T19:52:28.327141Z"
    },
    "papermill": {
     "duration": 0.003458,
     "end_time": "2025-01-01T19:52:28.327673",
     "exception": false,
     "start_time": "2025-01-01T19:52:28.324215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This approach has been scraped, since it creates hundreds of millions of edges and fills up my whole memory\n",
    "# The idea was to create edges between all nodes that share the same author, subreddit or search query\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def createedges_from_group(df, group_feature):\n",
    "#     \"\"\"Create edges by connecting all nodes within the same group without duplicates.\"\"\"\n",
    "#     edges = []\n",
    "#     unique_groups = sorted(df[group_feature].unique())  # Sort groups for ordered processing\n",
    "#\n",
    "#     for group in tqdm(unique_groups, desc=f\"Processing {group_feature}\"):\n",
    "#         # Filter IDs for the current group\n",
    "#         group_ids = sorted(df[df[group_feature] == group][\"id\"].to_numpy())  # Sort node IDs\n",
    "#\n",
    "#         # Process connections for each node\n",
    "#         while group_ids:  # Keep processing until all nodes in this group are handled\n",
    "#             current_node = group_ids.pop(0)  # Take the first node and \"pop\" it from the list\n",
    "#             for target_node in group_ids:  # Connect it with all remaining nodes\n",
    "#                 edges.append((current_node, target_node))\n",
    "#     return edges\n",
    "#\n",
    "#\n",
    "# # Same-author, same-subreddit, same-search_query edges (bidirectional)\n",
    "# edges_same_author = createedges_from_group(df_reddit, \"author\")\n",
    "# edges_same_subreddit = createedges_from_group(df_reddit, \"subreddit\")\n",
    "# edges_same_query = createedges_from_group(df_reddit, \"search_query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6be116d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:28.330184Z",
     "iopub.status.busy": "2025-01-01T19:52:28.330113Z",
     "iopub.status.idle": "2025-01-01T19:52:28.333438Z",
     "shell.execute_reply": "2025-01-01T19:52:28.333215Z"
    },
    "papermill": {
     "duration": 0.004976,
     "end_time": "2025-01-01T19:52:28.333768",
     "exception": false,
     "start_time": "2025-01-01T19:52:28.328792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, hidden_channels, out_channels, num_layers\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # GAT layers\n",
    "        self.gat = torch_geometric.nn.models.GAT(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_layers=num_layers,\n",
    "            v2=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply GAT layers\n",
    "        x = self.gat(x, edge_index)\n",
    "\n",
    "        # Return the output with a sigmoid activation\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class RedditDataset(Data):\n",
    "    def __init__(self, data, device) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.edge_index = None\n",
    "        self.device = device\n",
    "\n",
    "    def process(self) -> None:\n",
    "        # reset index\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "\n",
    "        # create mapping\n",
    "        id_mapping = dict(zip(self.data[\"id\"].values, self.data.index.values))\n",
    "\n",
    "        # replace hashes with indices\n",
    "        self.data[\"id\"] = self.data[\"id\"].map(id_mapping).astype(int)\n",
    "        self.data[\"parent_id\"] = (\n",
    "            self.data[\"parent_id\"].map(id_mapping).fillna(-1).astype(int)\n",
    "        )\n",
    "\n",
    "        # create edges\n",
    "        edges = pd.concat([self.data[\"id\"], self.data[\"parent_id\"]], axis=1)\n",
    "        edges = edges[edges[\"parent_id\"] != -1]\n",
    "        edges = edges.to_numpy()\n",
    "\n",
    "        # prepare data\n",
    "        self.x = torch.tensor(\n",
    "            np.array(self.data[\"features\"].to_list()), dtype=torch.float\n",
    "        ).to(self.device)\n",
    "        self.y = torch.tensor(\n",
    "            self.data[\"fraud\"].to_list(), dtype=torch.float\n",
    "        ).to(self.device)\n",
    "        self.edge_index = (\n",
    "            torch.tensor(edges, dtype=torch.long)\n",
    "            .t()\n",
    "            .contiguous()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb7779ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:28.336334Z",
     "iopub.status.busy": "2025-01-01T19:52:28.336252Z",
     "iopub.status.idle": "2025-01-01T19:52:28.337843Z",
     "shell.execute_reply": "2025-01-01T19:52:28.337647Z"
    },
    "papermill": {
     "duration": 0.003386,
     "end_time": "2025-01-01T19:52:28.338279",
     "exception": false,
     "start_time": "2025-01-01T19:52:28.334893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def balance_labels_in_dataset(df, logger_):\n",
    "#     \"\"\"Balance labels in the dataset by collecting enough non-fraud comments.\"\"\"\n",
    "#     n_datapoints_fraud = df[df[\"fraud\"] == 1].shape[0]\n",
    "#     all_post_ids = get_all_post_ids(df)\n",
    "#     comment_ids_non_fraud = set()\n",
    "#\n",
    "#     # Iterate over each post and collect children for non-fraud nodes\n",
    "#     for post_id in all_post_ids:\n",
    "#         nodes_to_visit = [post_id]\n",
    "#         visited = set()\n",
    "#\n",
    "#         # Traverse nodes iteratively to collect all children\n",
    "#         while nodes_to_visit:\n",
    "#             current_node = nodes_to_visit.pop()\n",
    "#             if current_node not in visited:\n",
    "#                 visited.add(current_node)\n",
    "#                 comment_ids_non_fraud.add(current_node)\n",
    "#\n",
    "#                 # Add children of the current node to the visit queue\n",
    "#                 children = df[df[\"parent_id\"] == current_node][\"id\"].to_list()\n",
    "#                 nodes_to_visit.extend(children)\n",
    "#\n",
    "#         # Stop early once enough non-fraud datapoints are collected\n",
    "#         if len(comment_ids_non_fraud) >= n_datapoints_fraud:\n",
    "#             break\n",
    "#\n",
    "#     logger_.debug(\n",
    "#         f\"Balancing labels: {n_datapoints_fraud} fraud datapoints, {len(comment_ids_non_fraud)} non-fraud datapoints\"\n",
    "#     )\n",
    "#\n",
    "#     # Concatenate fraud datapoints and balanced non-fraud datapoints\n",
    "#     df_balanced = pd.concat(\n",
    "#         [\n",
    "#             df[df[\"fraud\"] == 1],\n",
    "#             df[df[\"id\"].isin(comment_ids_non_fraud)],\n",
    "#         ]\n",
    "#     )\n",
    "#\n",
    "#     return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c4ee3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:28.341766Z",
     "iopub.status.busy": "2025-01-01T19:52:28.341561Z",
     "iopub.status.idle": "2025-01-01T19:52:28.348223Z",
     "shell.execute_reply": "2025-01-01T19:52:28.347986Z"
    },
    "papermill": {
     "duration": 0.008914,
     "end_time": "2025-01-01T19:52:28.348580",
     "exception": false,
     "start_time": "2025-01-01T19:52:28.339666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_post_ids(df, device):\n",
    "    \"\"\"Get all root post IDs (those with no parent).\"\"\"\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        return (\n",
    "            df[df[\"parent_id\"].isna()][\"id\"]\n",
    "            .sample(frac=1, random_state=42)\n",
    "            .to_arrow()\n",
    "            .to_pylist()\n",
    "        )\n",
    "    return (\n",
    "        df[df[\"parent_id\"].isna()][\"id\"]\n",
    "        .sample(frac=1, random_state=42)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "\n",
    "def balance_labels_in_dataset(df, device, logger_):\n",
    "    \"\"\"Balance labels in the dataset by collecting enough non-fraud comments.\"\"\"\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        logger_.debug(\"Converting DataFrame to cuDF\")\n",
    "        df = cudf.DataFrame.from_pandas(df)\n",
    "\n",
    "    n_datapoints_fraud = df[df[\"fraud\"] == 1].shape[0]\n",
    "    all_post_ids = get_all_post_ids(df, device)\n",
    "    comment_ids_non_fraud = set()\n",
    "\n",
    "    # Iterate over each post and collect children for non-fraud nodes\n",
    "    logger_.debug(\"Balancing labels: Collecting non-fraud comments\")\n",
    "    for post_id in all_post_ids:\n",
    "        nodes_to_visit = [post_id]\n",
    "        visited = set()\n",
    "\n",
    "        # Traverse nodes iteratively to collect all children\n",
    "        while nodes_to_visit:\n",
    "            current_node = nodes_to_visit.pop()\n",
    "            if current_node not in visited:\n",
    "                visited.add(current_node)\n",
    "                comment_ids_non_fraud.add(current_node)\n",
    "\n",
    "                # Add children of the current node to the visit queue\n",
    "                if device == torch.device(\"cuda\"):\n",
    "                    children = (\n",
    "                        df[df[\"parent_id\"] == current_node][\"id\"]\n",
    "                        .to_arrow()\n",
    "                        .to_pylist()\n",
    "                    )\n",
    "                else:\n",
    "                    children = df[df[\"parent_id\"] == current_node][\n",
    "                        \"id\"\n",
    "                    ].to_list()\n",
    "                nodes_to_visit.extend(children)\n",
    "\n",
    "        # Stop early once enough non-fraud datapoints are collected\n",
    "        if len(comment_ids_non_fraud) >= n_datapoints_fraud:\n",
    "            break\n",
    "\n",
    "    logger_.debug(\n",
    "        f\"Balanced labels: {n_datapoints_fraud} fraud datapoints, {len(comment_ids_non_fraud)} non-fraud datapoints\"\n",
    "    )\n",
    "\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        logger_.debug(\"Converting cuDF back to DataFrame\")\n",
    "        df = df.to_pandas()\n",
    "\n",
    "    # Concatenate fraud datapoints and balanced non-fraud datapoints\n",
    "    df_balanced = pd.concat(\n",
    "        [\n",
    "            df[df[\"fraud\"] == 1],\n",
    "            df[df[\"id\"].isin(comment_ids_non_fraud)],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Clear CUDA memory\n",
    "    del df\n",
    "\n",
    "    logger_.debug(f\"Balancing done.\")\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "\n",
    "def prepare_datasets(df, coin, device, balance_labels, logger_):\n",
    "    logger_.debug(f\"Preparing datasets for coin: {coin}\")\n",
    "    fit_data = df[df[\"search_query\"] != coin]\n",
    "    val_data = df[df[\"search_query\"] == coin]\n",
    "\n",
    "    if balance_labels:\n",
    "        logger_.debug(\"Balancing labels in the fitting dataset\")\n",
    "        fit_data = balance_labels_in_dataset(fit_data, device, logger_)\n",
    "\n",
    "    fit_dataset = RedditDataset(fit_data, device)\n",
    "    val_dataset = RedditDataset(val_data, device)\n",
    "\n",
    "    fit_dataset.process()\n",
    "    val_dataset.process()\n",
    "\n",
    "    return fit_dataset, val_dataset\n",
    "\n",
    "\n",
    "def prepare_model(\n",
    "    device, in_channels, hidden_channels, out_channels, num_layers, lr, logger_\n",
    "):\n",
    "    \"\"\"Initialize the GAT model, loss function, and optimizer.\"\"\"\n",
    "    logger_.debug(\n",
    "        \"Initializing model with parameters: \"\n",
    "        f\"in_channels={in_channels}, hidden_channels={hidden_channels}, \"\n",
    "        f\"out_channels={out_channels}, num_layers={num_layers}, lr={lr}\"\n",
    "    )\n",
    "    model = GAT(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=hidden_channels,\n",
    "        out_channels=out_channels,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "\n",
    "def train_model(model, dataset, criterion, optimizer, logger_):\n",
    "    \"\"\"Train the model for one iteration.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset.x, dataset.edge_index).squeeze()\n",
    "    loss = criterion(out, dataset.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    logger_.debug(f\"Training loss: {loss.item()}\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def validate_model(model, dataset, criterion, logger_):\n",
    "    \"\"\"Evaluate the model on the validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(dataset.x, dataset.edge_index).squeeze()\n",
    "        val_loss = criterion(out, dataset.y)\n",
    "        all_preds.append(out)\n",
    "        all_targets.append(dataset.y)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "    accuracy = ((all_preds >= 0.5) * 1 == all_targets).mean()\n",
    "\n",
    "    logger_.debug(\n",
    "        f\"Validation loss: {val_loss.item()}, Validation accuracy: {accuracy}\"\n",
    "    )\n",
    "    return val_loss, accuracy\n",
    "\n",
    "\n",
    "def train_for_coin(\n",
    "    coin,\n",
    "    df,\n",
    "    device,\n",
    "    training_loops,\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    num_layers,\n",
    "    lr,\n",
    "    balance_labels,\n",
    "    logger_,\n",
    "):\n",
    "    \"\"\"Train and validate the model, leaving one coin out.\"\"\"\n",
    "    logger_.debug(f\"Training model - leaving out {coin}\")\n",
    "\n",
    "    # Prepare data\n",
    "    fit_dataset, val_dataset = prepare_datasets(\n",
    "        df, coin, device, balance_labels, logger_\n",
    "    )\n",
    "\n",
    "    # Prepare model\n",
    "    model, criterion, optimizer = prepare_model(\n",
    "        device,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        lr,\n",
    "        logger_,\n",
    "    )\n",
    "\n",
    "    for i in range(training_loops):\n",
    "        logger_.debug(f\"Training loop {i + 1}/{training_loops}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_model(\n",
    "            model, fit_dataset, criterion, optimizer, logger_\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        val_loss, accuracy = validate_model(\n",
    "            model, val_dataset, criterion, logger_\n",
    "        )\n",
    "        logger_.debug(\n",
    "            f\"End of loop {i + 1}: Training loss: {train_loss.item()}, Validation loss: {val_loss.item()}, Validation accuracy: {accuracy}\"\n",
    "        )\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main(\n",
    "    df,\n",
    "    device,\n",
    "    training_loops,\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    num_layers,\n",
    "    lr,\n",
    "    balance_labels,\n",
    "    logger_,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a GAT model for each coin in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): Input dataset.\n",
    "        device (torch.device): Device for training.\n",
    "        training_loops (int): Number of training loops.\n",
    "        in_channels (int): Input channels for the GAT model.\n",
    "        hidden_channels (int): Number of hidden channels in the GAT model.\n",
    "        out_channels (int): Number of output channels for the GAT model.\n",
    "        num_layers (int): Number of layers in the GAT model.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        balance_labels (bool): Whether to balance the labels in the fitting dataset.\n",
    "        logger_ (logging.Logger): Logger object for logging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: A dictionary where keys are coin names and values are validation accuracies.\n",
    "\n",
    "    \"\"\"\n",
    "    accuracies = {}\n",
    "    coins = df[\"search_query\"].unique()\n",
    "\n",
    "    for coin in coins:\n",
    "        accuracy = train_for_coin(\n",
    "            coin,\n",
    "            df,\n",
    "            device,\n",
    "            training_loops,\n",
    "            in_channels,\n",
    "            hidden_channels,\n",
    "            out_channels,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            balance_labels,\n",
    "            logger_,\n",
    "        )\n",
    "        accuracies[coin] = accuracy.item()\n",
    "\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ffd7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:28.352478Z",
     "iopub.status.busy": "2025-01-01T19:52:28.352149Z",
     "iopub.status.idle": "2025-01-01T19:52:34.292679Z",
     "shell.execute_reply": "2025-01-01T19:52:34.292456Z"
    },
    "papermill": {
     "duration": 5.943044,
     "end_time": "2025-01-01T19:52:34.293100",
     "exception": false,
     "start_time": "2025-01-01T19:52:28.350056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Test if works\n",
    "_ = main(\n",
    "    df_train,\n",
    "    device,\n",
    "    training_loops=10,\n",
    "    in_channels=769,\n",
    "    hidden_channels=4,\n",
    "    out_channels=1,\n",
    "    num_layers=2,\n",
    "    lr=0.0005,\n",
    "    balance_labels=False,\n",
    "    logger_=logger_,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2742962c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:34.303718Z",
     "iopub.status.busy": "2025-01-01T19:52:34.303545Z",
     "iopub.status.idle": "2025-01-01T19:52:34.306799Z",
     "shell.execute_reply": "2025-01-01T19:52:34.306594Z"
    },
    "papermill": {
     "duration": 0.012502,
     "end_time": "2025-01-01T19:52:34.307137",
     "exception": false,
     "start_time": "2025-01-01T19:52:34.294635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_json_file(file_path):\n",
    "    \"\"\"Initialize the JSON file if it does not exist.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        with file_path.open(\"w\") as f:\n",
    "            json.dump([], f)  # Start with an empty list\n",
    "\n",
    "\n",
    "def append_to_json(file_path, data):\n",
    "    \"\"\"Append a new entry to the JSON file.\"\"\"\n",
    "    with file_path.open(\"r+\") as f:\n",
    "        existing_data = json.load(f)  # Load existing data\n",
    "        existing_data.append(data)  # Add the new entry\n",
    "        f.seek(0)  # Reset cursor to the beginning of the file\n",
    "        json.dump(\n",
    "            existing_data, f, indent=4\n",
    "        )  # Write updated data back to the file\n",
    "\n",
    "\n",
    "def perform_hyperparameter_search(\n",
    "    searches, file_path, logger_, df_train, device, main\n",
    "):\n",
    "    \"\"\"Perform random hyperparameter searches and log results.\"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for i in range(searches):\n",
    "        try:\n",
    "            # Randomly sample hyperparameters\n",
    "            params = {\n",
    "                \"training_loops\": rng.choice([4000, 8000]).item(),\n",
    "                \"hidden_channels\": rng.choice(\n",
    "                    [8, 16, 32, 64, 128, 256]\n",
    "                ).item(),\n",
    "                \"num_layers\": rng.integers(1, 10).item(),\n",
    "                \"lr\": rng.choice([0.0001, 0.0005, 0.001, 0.005]).item(),\n",
    "                \"balance_labels\": True,\n",
    "            }\n",
    "            logger_.info(f\"Starting search {i + 1}/{searches}\")\n",
    "            logger_.info(f\"Hyperparameters: {params}\")\n",
    "\n",
    "            # Run the main function to get accuracies\n",
    "            accuracies = main(\n",
    "                df_train,\n",
    "                device,\n",
    "                training_loops=params[\"training_loops\"],\n",
    "                in_channels=769,\n",
    "                hidden_channels=params[\"hidden_channels\"],\n",
    "                out_channels=1,\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                lr=params[\"lr\"],\n",
    "                balance_labels=params[\"balance_labels\"],\n",
    "                logger_=logger_,\n",
    "            )\n",
    "\n",
    "            # Combine parameters and accuracies\n",
    "            result = {**params, \"accuracies\": accuracies}\n",
    "\n",
    "            # Append result to the JSON file\n",
    "            append_to_json(file_path, result)\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError as e:  # noqa: PERF203\n",
    "            logger_.error(\n",
    "                f\"Out of memory error: {e}, skipping search {i + 1}/{searches}\"\n",
    "            )\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea49a898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T19:52:34.309879Z",
     "iopub.status.busy": "2025-01-01T19:52:34.309800Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-01-01T19:52:34.308376",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 20:52:34,310 - graph_attention_network - INFO - Starting search 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 20:52:34,310 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 4000, 'hidden_channels': 32, 'num_layers': 9, 'lr': 0.001, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 21:03:52,727 - graph_attention_network - INFO - Starting search 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 21:03:52,728 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 8000, 'hidden_channels': 256, 'num_layers': 7, 'lr': 0.005, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:03:08,166 - graph_attention_network - INFO - Starting search 3/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:03:08,166 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 4000, 'hidden_channels': 16, 'num_layers': 2, 'lr': 0.0001, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:10:21,994 - graph_attention_network - INFO - Starting search 4/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:10:21,994 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 4000, 'hidden_channels': 8, 'num_layers': 3, 'lr': 0.005, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:17:57,969 - graph_attention_network - INFO - Starting search 5/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:17:57,970 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 8000, 'hidden_channels': 16, 'num_layers': 1, 'lr': 0.0001, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:25:36,792 - graph_attention_network - INFO - Starting search 6/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:25:36,792 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 4000, 'hidden_channels': 8, 'num_layers': 8, 'lr': 0.005, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:35:32,067 - graph_attention_network - INFO - Starting search 7/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:35:32,067 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 8000, 'hidden_channels': 128, 'num_layers': 2, 'lr': 0.0001, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:47:27,345 - graph_attention_network - INFO - Starting search 8/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:47:27,345 - graph_attention_network - INFO - Hyperparameters: {'training_loops': 8000, 'hidden_channels': 16, 'num_layers': 2, 'lr': 0.005, 'balance_labels': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 22:56:17,806 - graph_attention_network - INFO - Starting search 9/1000\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "file_path = Path(\"../reports/hyperparameter_search_gat_v2.json\")\n",
    "initialize_json_file(file_path)\n",
    "\n",
    "perform_hyperparameter_search(\n",
    "    searches=1000,\n",
    "    file_path=file_path,\n",
    "    logger_=logger_,\n",
    "    df_train=df_train,\n",
    "    device=device,\n",
    "    main=main,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "52-gan.ipynb",
   "output_path": "52-gan.ipynb",
   "parameters": {},
   "start_time": "2025-01-01T19:52:11.312127",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
