{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532899a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:32.891470Z",
     "iopub.status.busy": "2024-12-31T19:15:32.891327Z",
     "iopub.status.idle": "2024-12-31T19:15:34.623685Z",
     "shell.execute_reply": "2024-12-31T19:15:34.623402Z"
    },
    "papermill": {
     "duration": 1.73473,
     "end_time": "2024-12-31T19:15:34.624450",
     "exception": false,
     "start_time": "2024-12-31T19:15:32.889720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from CryptoFraudDetection.utils.enums import LoggerMode\n",
    "from CryptoFraudDetection.utils.logger import Logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "logger_ = Logger(name=\"graph_attention_network\", level=LoggerMode.WARNING, log_dir=\"../logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003fb9ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:34.634735Z",
     "iopub.status.busy": "2024-12-31T19:15:34.634513Z",
     "iopub.status.idle": "2024-12-31T19:15:44.367669Z",
     "shell.execute_reply": "2024-12-31T19:15:44.367371Z"
    },
    "papermill": {
     "duration": 9.742646,
     "end_time": "2024-12-31T19:15:44.368320",
     "exception": false,
     "start_time": "2024-12-31T19:15:34.625674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_exists = None\n",
    "df_reddit = None\n",
    "\n",
    "try:\n",
    "    df_reddit = pd.read_parquet(\n",
    "        \"../data/processed/reddit_posts_embedded.parquet\",\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"parent_id\",\n",
    "            \"author\",\n",
    "            \"score\",\n",
    "            \"search_query\",\n",
    "            \"subreddit\",\n",
    "            \"test\",\n",
    "            \"fraud\",\n",
    "            \"embedding\",\n",
    "        ],\n",
    "    )\n",
    "    embedding_exists = True\n",
    "except FileNotFoundError:\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc400a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:44.378546Z",
     "iopub.status.busy": "2024-12-31T19:15:44.378447Z",
     "iopub.status.idle": "2024-12-31T19:15:44.384201Z",
     "shell.execute_reply": "2024-12-31T19:15:44.384049Z"
    },
    "papermill": {
     "duration": 0.015062,
     "end_time": "2024-12-31T19:15:44.384610",
     "exception": false,
     "start_time": "2024-12-31T19:15:44.369548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>search_query</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>test</th>\n",
       "      <th>fraud</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yxu5tv</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>magus-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>Safe Moon</td>\n",
       "      <td>r/CryptoCurrency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[-0.4334820508956909, -0.4628458321094513, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id parent_id    author  score search_query         subreddit  test  \\\n",
       "0  yxu5tv      <NA>  magus-21   1597    Safe Moon  r/CryptoCurrency  True   \n",
       "\n",
       "   fraud                                          embedding  \n",
       "0   True  [-0.4334820508956909, -0.4628458321094513, -0....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1eed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:44.386942Z",
     "iopub.status.busy": "2024-12-31T19:15:44.386779Z",
     "iopub.status.idle": "2024-12-31T19:15:44.398419Z",
     "shell.execute_reply": "2024-12-31T19:15:44.398229Z"
    },
    "papermill": {
     "duration": 0.013159,
     "end_time": "2024-12-31T19:15:44.398753",
     "exception": false,
     "start_time": "2024-12-31T19:15:44.385594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    502081.0\n",
       "mean     0.052482\n",
       "std      0.003728\n",
       "min           0.0\n",
       "25%      0.052263\n",
       "50%      0.052285\n",
       "75%      0.052328\n",
       "max           1.0\n",
       "Name: normalized_score, dtype: Float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_min = df_reddit[\"score\"].min()\n",
    "score_max = df_reddit[\"score\"].max()\n",
    "df_reddit[\"normalized_score\"] = (df_reddit[\"score\"] - score_min) / (score_max - score_min)\n",
    "df_reddit[\"normalized_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3917f0b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:44.401200Z",
     "iopub.status.busy": "2024-12-31T19:15:44.401031Z",
     "iopub.status.idle": "2024-12-31T19:15:47.870093Z",
     "shell.execute_reply": "2024-12-31T19:15:47.869777Z"
    },
    "papermill": {
     "duration": 3.47073,
     "end_time": "2024-12-31T19:15:47.870520",
     "exception": false,
     "start_time": "2024-12-31T19:15:44.399790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>search_query</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>test</th>\n",
       "      <th>fraud</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yxu5tv</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>magus-21</td>\n",
       "      <td>1597</td>\n",
       "      <td>Safe Moon</td>\n",
       "      <td>r/CryptoCurrency</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[0.08661773252685279, -0.4334820508956909, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id parent_id    author  score search_query         subreddit  test  \\\n",
       "0  yxu5tv      <NA>  magus-21   1597    Safe Moon  r/CryptoCurrency  True   \n",
       "\n",
       "   fraud                                           features  \n",
       "0   True  [0.08661773252685279, -0.4334820508956909, -0....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add features column\n",
    "df_reddit[\"features\"] = df_reddit.apply(\n",
    "    lambda row: np.concatenate([np.array([row[\"normalized_score\"]]), np.array(row[\"embedding\"])], axis=0),\n",
    "    axis=1,\n",
    ")\n",
    "df_reddit = df_reddit.drop(columns=[\"embedding\", \"normalized_score\"])\n",
    "\n",
    "df_reddit.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ca8b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:47.880942Z",
     "iopub.status.busy": "2024-12-31T19:15:47.880799Z",
     "iopub.status.idle": "2024-12-31T19:15:47.930725Z",
     "shell.execute_reply": "2024-12-31T19:15:47.930417Z"
    },
    "papermill": {
     "duration": 0.059681,
     "end_time": "2024-12-31T19:15:47.931554",
     "exception": false,
     "start_time": "2024-12-31T19:15:47.871873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_reddit[~df_reddit[\"test\"]]\n",
    "df_test = df_reddit[df_reddit[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45362a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:47.934372Z",
     "iopub.status.busy": "2024-12-31T19:15:47.934262Z",
     "iopub.status.idle": "2024-12-31T19:15:47.935936Z",
     "shell.execute_reply": "2024-12-31T19:15:47.935784Z"
    },
    "papermill": {
     "duration": 0.003467,
     "end_time": "2024-12-31T19:15:47.936311",
     "exception": false,
     "start_time": "2024-12-31T19:15:47.932844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This approach has been scraped, since it creates hundreds of millions of edges and fills up my whole memory\n",
    "# The idea was to create edges between all nodes that share the same author, subreddit or search query\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def createedges_from_group(df, group_feature):\n",
    "#     \"\"\"Create edges by connecting all nodes within the same group without duplicates.\"\"\"\n",
    "#     edges = []\n",
    "#     unique_groups = sorted(df[group_feature].unique())  # Sort groups for ordered processing\n",
    "#\n",
    "#     for group in tqdm(unique_groups, desc=f\"Processing {group_feature}\"):\n",
    "#         # Filter IDs for the current group\n",
    "#         group_ids = sorted(df[df[group_feature] == group][\"id\"].to_numpy())  # Sort node IDs\n",
    "#\n",
    "#         # Process connections for each node\n",
    "#         while group_ids:  # Keep processing until all nodes in this group are handled\n",
    "#             current_node = group_ids.pop(0)  # Take the first node and \"pop\" it from the list\n",
    "#             for target_node in group_ids:  # Connect it with all remaining nodes\n",
    "#                 edges.append((current_node, target_node))\n",
    "#     return edges\n",
    "#\n",
    "#\n",
    "# # Same-author, same-subreddit, same-search_query edges (bidirectional)\n",
    "# edges_same_author = createedges_from_group(df_reddit, \"author\")\n",
    "# edges_same_subreddit = createedges_from_group(df_reddit, \"subreddit\")\n",
    "# edges_same_query = createedges_from_group(df_reddit, \"search_query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6be116d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:47.938653Z",
     "iopub.status.busy": "2024-12-31T19:15:47.938573Z",
     "iopub.status.idle": "2024-12-31T19:15:47.942647Z",
     "shell.execute_reply": "2024-12-31T19:15:47.942492Z"
    },
    "papermill": {
     "duration": 0.005616,
     "end_time": "2024-12-31T19:15:47.942933",
     "exception": false,
     "start_time": "2024-12-31T19:15:47.937317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # GAT layers\n",
    "        self.gat = torch_geometric.nn.models.GAT(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            num_layers=num_layers,\n",
    "            v2=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply GAT layers\n",
    "        x = self.gat(x, edge_index)\n",
    "\n",
    "        # Return the output with a sigmoid activation\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class RedditDataset(Data):\n",
    "    def __init__(self, data, device) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.edge_index = None\n",
    "        self.device = device\n",
    "\n",
    "    def process(self) -> None:\n",
    "        # reset index\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "\n",
    "        # create mapping\n",
    "        id_mapping = dict(zip(self.data[\"id\"].values, self.data.index.values))\n",
    "\n",
    "        # replace hashes with indices\n",
    "        self.data[\"id\"] = self.data[\"id\"].map(id_mapping).astype(int)\n",
    "        self.data[\"parent_id\"] = self.data[\"parent_id\"].map(id_mapping).fillna(-1).astype(int)\n",
    "\n",
    "        # create edges\n",
    "        edges = pd.concat([self.data[\"id\"], self.data[\"parent_id\"]], axis=1)\n",
    "        edges = edges[edges[\"parent_id\"] != -1]\n",
    "        edges = edges.to_numpy()\n",
    "\n",
    "        # prepare data\n",
    "        self.x = torch.tensor(np.array(self.data[\"features\"].to_list()), dtype=torch.float).to(self.device)\n",
    "        self.y = torch.tensor(self.data[\"fraud\"].to_list(), dtype=torch.float).to(self.device)\n",
    "        self.edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(self.device)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4ee3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:47.945415Z",
     "iopub.status.busy": "2024-12-31T19:15:47.945328Z",
     "iopub.status.idle": "2024-12-31T19:15:47.949920Z",
     "shell.execute_reply": "2024-12-31T19:15:47.949753Z"
    },
    "papermill": {
     "duration": 0.006344,
     "end_time": "2024-12-31T19:15:47.950283",
     "exception": false,
     "start_time": "2024-12-31T19:15:47.943939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(df, coin, device, balance_labels, logger_):\n",
    "    \"\"\"Prepare training and validation datasets.\"\"\"\n",
    "    logger_.debug(f\"Preparing datasets for coin: {coin}\")\n",
    "    fit_data = df[df[\"search_query\"] != coin]\n",
    "\n",
    "    if balance_labels:\n",
    "        fit_data = pd.concat(\n",
    "            [\n",
    "                fit_data[fit_data[\"fraud\"] == 1],\n",
    "                fit_data[fit_data[\"fraud\"] == 0].sample(n=fit_data[fit_data[\"fraud\"] == 1].shape[0], random_state=42),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    val_data = df[df[\"search_query\"] == coin]\n",
    "\n",
    "    fit_dataset = RedditDataset(fit_data, device)\n",
    "    fit_dataset.process()\n",
    "\n",
    "    val_dataset = RedditDataset(val_data, device)\n",
    "    val_dataset.process()\n",
    "\n",
    "    return fit_dataset, val_dataset\n",
    "\n",
    "\n",
    "def prepare_model(device, in_channels, hidden_channels, out_channels, num_layers, lr, logger_):\n",
    "    \"\"\"Initialize the GAT model, loss function, and optimizer.\"\"\"\n",
    "    logger_.debug(\n",
    "        \"Initializing model with parameters: \"\n",
    "        f\"in_channels={in_channels}, hidden_channels={hidden_channels}, \"\n",
    "        f\"out_channels={out_channels}, num_layers={num_layers}, lr={lr}\"\n",
    "    )\n",
    "    model = GAT(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=hidden_channels,\n",
    "        out_channels=out_channels,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "\n",
    "def train_model(model, dataset, criterion, optimizer, logger_):\n",
    "    \"\"\"Train the model for one iteration.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset.x, dataset.edge_index).squeeze()\n",
    "    loss = criterion(out, dataset.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    logger_.debug(f\"Training loss: {loss.item()}\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def validate_model(model, dataset, criterion, logger_):\n",
    "    \"\"\"Evaluate the model on the validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(dataset.x, dataset.edge_index).squeeze()\n",
    "        val_loss = criterion(out, dataset.y)\n",
    "        all_preds.append(out)\n",
    "        all_targets.append(dataset.y)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "    accuracy = ((all_preds >= 0.5) * 1 == all_targets).mean()\n",
    "\n",
    "    logger_.debug(f\"Validation loss: {val_loss.item()}, Validation accuracy: {accuracy}\")\n",
    "    return val_loss, accuracy\n",
    "\n",
    "\n",
    "def train_for_coin(\n",
    "    coin,\n",
    "    df,\n",
    "    device,\n",
    "    training_loops,\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    num_layers,\n",
    "    lr,\n",
    "    balance_labels,\n",
    "    logger_,\n",
    "):\n",
    "    \"\"\"Train and validate the model, leaving one coin out.\"\"\"\n",
    "    logger_.debug(f\"Training model - leaving out {coin}\")\n",
    "\n",
    "    # Prepare data\n",
    "    fit_dataset, val_dataset = prepare_datasets(df, coin, device, balance_labels, logger_)\n",
    "\n",
    "    # Prepare model\n",
    "    model, criterion, optimizer = prepare_model(\n",
    "        device,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        lr,\n",
    "        logger_,\n",
    "    )\n",
    "\n",
    "    for i in range(training_loops):\n",
    "        logger_.debug(f\"Training loop {i + 1}/{training_loops}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss = train_model(model, fit_dataset, criterion, optimizer, logger_)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, accuracy = validate_model(model, val_dataset, criterion, logger_)\n",
    "        logger_.debug(\n",
    "            f\"End of loop {i + 1}: Training loss: {train_loss.item()}, Validation loss: {val_loss.item()}, Validation accuracy: {accuracy}\"\n",
    "        )\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main(\n",
    "    df,\n",
    "    device,\n",
    "    training_loops,\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    num_layers,\n",
    "    lr,\n",
    "    balance_labels,\n",
    "    logger_,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a GAT model for each coin in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df (DataFrame): Input dataset.\n",
    "        device (torch.device): Device for training.\n",
    "        training_loops (int): Number of training loops.\n",
    "        in_channels (int): Input channels for the GAT model.\n",
    "        hidden_channels (int): Number of hidden channels in the GAT model.\n",
    "        out_channels (int): Number of output channels for the GAT model.\n",
    "        num_layers (int): Number of layers in the GAT model.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        balance_labels (bool): Whether to balance the labels in the fitting dataset.\n",
    "        logger_ (logging.Logger): Logger object for logging.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: A dictionary where keys are coin names and values are validation accuracies.\n",
    "\n",
    "    \"\"\"\n",
    "    accuracies = {}\n",
    "    coins = df[\"search_query\"].unique()\n",
    "\n",
    "    for coin in coins:\n",
    "        accuracy = train_for_coin(\n",
    "            coin,\n",
    "            df,\n",
    "            device,\n",
    "            training_loops,\n",
    "            in_channels,\n",
    "            hidden_channels,\n",
    "            out_channels,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            balance_labels,\n",
    "            logger_,\n",
    "        )\n",
    "        accuracies[coin] = accuracy.item()\n",
    "\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ffd7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:47.952801Z",
     "iopub.status.busy": "2024-12-31T19:15:47.952600Z",
     "iopub.status.idle": "2024-12-31T19:15:55.339232Z",
     "shell.execute_reply": "2024-12-31T19:15:55.338974Z"
    },
    "papermill": {
     "duration": 7.388287,
     "end_time": "2024-12-31T19:15:55.339647",
     "exception": false,
     "start_time": "2024-12-31T19:15:47.951360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Test if works\n",
    "_ = main(\n",
    "    df_train,\n",
    "    device,\n",
    "    training_loops=10,\n",
    "    in_channels=769,\n",
    "    hidden_channels=256,\n",
    "    out_channels=1,\n",
    "    num_layers=10,\n",
    "    lr=0.0005,\n",
    "    balance_labels=True,\n",
    "    logger_=logger_,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2742962c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T19:15:55.350056Z",
     "iopub.status.busy": "2024-12-31T19:15:55.349969Z",
     "iopub.status.idle": "2025-01-01T11:30:18.959209Z",
     "shell.execute_reply": "2025-01-01T11:30:18.958809Z"
    },
    "papermill": {
     "duration": 58463.228131,
     "end_time": "2025-01-01T11:30:18.569136",
     "exception": false,
     "start_time": "2024-12-31T19:15:55.341005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 00:30:28,804 - graph_attention_network - ERROR - Out of memory error: CUDA out of memory. Tried to allocate 478.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 11.46 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), skipping search 37/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 05:39:15,316 - graph_attention_network - ERROR - Out of memory error: CUDA out of memory. Tried to allocate 478.00 MiB. GPU 0 has a total capacity of 11.60 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 11.46 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), skipping search 72/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../reports/hyperparameter_search_gat.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m initialize_json_file(file_path)\n\u001b[0;32m---> 64\u001b[0m \u001b[43mperform_hyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36mperform_hyperparameter_search\u001b[0;34m(searches, file_path, logger_, df_train, device, main)\u001b[0m\n\u001b[1;32m     32\u001b[0m logger_\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run the main function to get accuracies\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_loops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_loops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m769\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_channels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbalance_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalance_labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Combine parameters and accuracies\u001b[39;00m\n\u001b[1;32m     49\u001b[0m result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracies\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracies}\n",
      "Cell \u001b[0;32mIn[9], line 161\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(df, device, training_loops, in_channels, hidden_channels, out_channels, num_layers, lr, balance_labels, logger_)\u001b[0m\n\u001b[1;32m    158\u001b[0m coins \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_query\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coin \u001b[38;5;129;01min\u001b[39;00m coins:\n\u001b[0;32m--> 161\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_coin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_loops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbalance_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     accuracies[coin] \u001b[38;5;241m=\u001b[39m accuracy\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracies\n",
      "Cell \u001b[0;32mIn[9], line 113\u001b[0m, in \u001b[0;36mtrain_for_coin\u001b[0;34m(coin, df, device, training_loops, in_channels, hidden_channels, out_channels, num_layers, lr, balance_labels, logger_)\u001b[0m\n\u001b[1;32m    110\u001b[0m logger_\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_loops\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m    116\u001b[0m val_loss, accuracy \u001b[38;5;241m=\u001b[39m validate_model(model, val_dataset, criterion, logger_)\n",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, criterion, optimizer, logger_)\u001b[0m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, dataset\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     51\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Apply GAT layers\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Return the output with a sigmoid activation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch_geometric/nn/models/basic_gnn.py:254\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[0;34m(self, x, edge_index, edge_weight, edge_attr, batch, batch_size, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[1;32m    252\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x, edge_index, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_edge_attr:\n\u001b[0;32m--> 254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x, edge_index)\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch_geometric/nn/conv/gatv2_conv.py:310\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_nodes, x_r\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 310\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mremove_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m add_self_loops(\n\u001b[1;32m    313\u001b[0m         edge_index, edge_attr, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value,\n\u001b[1;32m    314\u001b[0m         num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch_geometric/utils/loop.py:115\u001b[0m, in \u001b[0;36mremove_self_loops\u001b[0;34m(edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    112\u001b[0m mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    113\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[1;32m    116\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/com.github/CryptoFraudDetection/main/.venv/lib/python3.12/site-packages/torch/_jit_internal.py:103\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m                return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def initialize_json_file(file_path):\n",
    "    \"\"\"Initialize the JSON file if it does not exist.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        with file_path.open(\"w\") as f:\n",
    "            json.dump([], f)  # Start with an empty list\n",
    "\n",
    "\n",
    "def append_to_json(file_path, data):\n",
    "    \"\"\"Append a new entry to the JSON file.\"\"\"\n",
    "    with file_path.open(\"r+\") as f:\n",
    "        existing_data = json.load(f)  # Load existing data\n",
    "        existing_data.append(data)  # Add the new entry\n",
    "        f.seek(0)  # Reset cursor to the beginning of the file\n",
    "        json.dump(existing_data, f, indent=4)  # Write updated data back to the file\n",
    "\n",
    "\n",
    "def perform_hyperparameter_search(searches, file_path, logger_, df_train, device, main):\n",
    "    \"\"\"Perform random hyperparameter searches and log results.\"\"\"\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for i in range(searches):\n",
    "        try:\n",
    "            # Randomly sample hyperparameters\n",
    "            params = {\n",
    "                \"training_loops\": rng.choice([500, 1000, 2000, 4000]).item(),\n",
    "                \"hidden_channels\": rng.choice([8, 16, 32, 64, 128, 256]).item(),\n",
    "                \"num_layers\": rng.integers(1, 10).item(),\n",
    "                \"lr\": rng.choice([0.0001, 0.0005, 0.001, 0.005]).item(),\n",
    "                \"balance_labels\": rng.choice([True, False]).item(),\n",
    "            }\n",
    "            logger_.info(f\"Starting search {i + 1}/{searches}\")\n",
    "            logger_.info(f\"Hyperparameters: {params}\")\n",
    "\n",
    "            # Run the main function to get accuracies\n",
    "            accuracies = main(\n",
    "                df_train,\n",
    "                device,\n",
    "                training_loops=params[\"training_loops\"],\n",
    "                in_channels=769,\n",
    "                hidden_channels=params[\"hidden_channels\"],\n",
    "                out_channels=1,\n",
    "                num_layers=params[\"num_layers\"],\n",
    "                lr=params[\"lr\"],\n",
    "                balance_labels=params[\"balance_labels\"],\n",
    "                logger_=logger_,\n",
    "            )\n",
    "\n",
    "            # Combine parameters and accuracies\n",
    "            result = {**params, \"accuracies\": accuracies}\n",
    "\n",
    "            # Append result to the JSON file\n",
    "            append_to_json(file_path, result)\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError as e:  # noqa: PERF203\n",
    "            logger_.error(f\"Out of memory error: {e}, skipping search {i + 1}/{searches}\")\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "file_path = Path(\"../reports/hyperparameter_search_gat.json\")\n",
    "initialize_json_file(file_path)\n",
    "\n",
    "perform_hyperparameter_search(\n",
    "    searches=1000,\n",
    "    file_path=file_path,\n",
    "    logger_=logger_,\n",
    "    df_train=df_train,\n",
    "    device=device,\n",
    "    main=main,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 58487.933237,
   "end_time": "2025-01-01T11:30:20.185540",
   "environment_variables": {},
   "exception": null,
   "input_path": "52-gan.ipynb",
   "output_path": "52-gan.ipynb",
   "parameters": {},
   "start_time": "2024-12-31T19:15:32.252303",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}